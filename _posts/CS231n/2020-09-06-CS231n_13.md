---
title : "CS231n 13강 요약"
category :
    - CS231n
tag :
    - machine_learning
    - computer vision
    - CS231n
toc : true
toc_sticky: true
comments: true
---

Unsupervised Learning과 Generative Models에 대해서 알아보자!

## 0. Today's Goal
> - Unsupervised Learning
> - Generative Models

## 1. 강의 및 강의 자료

강의를 들어볼 수 있는 링크와 강의 자료를 pdf형식으로 준비했다.

- 강의 링크 : [video](https://www.youtube.com/watch?v=5WoItGTWV54&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=13)

- 강의 슬라이드 : [슬라이드](https://github.com/Taeyoung96/Taeyoung96.github.io/files/5179124/cs231n_2017_lecture13.pdf)

한글 자막이 필요하신 분은 다음의 링크를 확인해주세요. :)

- 한글 자막을 첨부하고 싶으면 [여기](https://github.com/visionNoob/CS231N_17_KOR_SUB)를 눌러주세요.

## 2. Unsupervised Learning

지금까지는 Supervised Learning에 대해서 주로 배웠다.  

Supervised Learning은 Data가 (x,y) 형태로 x는 data, y는 label일 때,  

x >> y를 Mapping 시켜주는 함수를 찾는 것을 목표로 했다.  

그 예로 지금까지 우리가 배웠던  
> - Classification
> - Regression
> - Object Detection
> - Semantic Segmentation
> - Image Captioning

등이 있다.  

그렇다면 Unsupervised Learning이란 무엇일까?  

Unsupervised Learning은 Data가 Label 없이 주어져 있을 때

**데이터의 숨어있는 기본적인 구조를 학습하는 것을 목표로 학습하는 방법**이다.  

그 예로는 
> - Clustering(군집화)
> - Dimensionality Reduction
> - Feature Learning
> - Density Estimation

등이 있다.  

예시를 그림으로 나타내보면  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92319406-bdc4d780-f052-11ea-9dca-cc7f7263a784.png" width = "600" ></p>

Unsupervised Learning이 가지는 장점은  
**데이터를 많이 모을 수 있다는 것**이다.  

하지만 확실한 학습 결과로 분류 기준을 사람이 알 수 없다는 것이 단점이다.  

## 3. Generative Models

생성 모델이란 Training Data를 넣어 줬을 때,  
동일한 분포에서 새로운 샘플들을 생성하는 것을 말한다.  

내가 이해하기론 Training Data와 비슷한 모양의 그림을 만들어 내는 것으로 이해했다.  

Generative Models도 Unsupervised Learning에 속하는데  
분포 추정(Density estimation)하는 것이 여기서 핵심 문제이다.  

Density estimation를 하는 방법에는 2가지가 있다.  

> 1. Explicit Density Estimation : 생성 모델 $P_{model}(x)$를 명시적으로 나타내 주는 방법
> 2. Implicit Density Estimation : 생성 모델 $P_{model}(x)$를 정의하지 않고 Sample을 얻어 내는 방법  


그렇다면 Generative Models을 이용하여 얻을 수 있는 것은 무엇인가?  

예를 들어,  
> 1. Samples for artwork
> 2. Super-resolution
> 3. Colorization
> 4. 밑그림 만으로 가방 만들어내기

등에 이용될 수 있다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92320007-08e1e900-f059-11ea-9d2a-643ede8faba1.png" width = "500" ></p>


Generative Models의 종류를 나타내면 아래 슬라이드와 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92320022-2fa01f80-f059-11ea-94e7-999250a52d12.png" width = "500" ></p>

오늘 강의에서는 Generative Models의 많은 종류 중  
> - PixelRNN/CNN
> - Variational Autoencoder
> - GAN

에 대해서 설명을 한다.  

### PixelRNN/CNN

PixelRNN/CNN은 Explicit Density Estimation의 중 계산 가능한 density를 다루는 방법 중 하나이다.  

Explicit Density Model을 만들기 위해서  

Chain Rule을 이용하여 Image x를 1차원 분포들 간의 곱의 형태로 분해를 한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92320776-f23e9080-f05e-11ea-95de-dd9e0fc9314c.png" width = "400" ></p>

Pixel Value간 복잡한 분포들은 Neural Network를 이용하여 표현한다!  

그렇다면 여기서 생기는 Issue는 '우리가 이전 Pixel들을 어떻게 정의할 것인가?'이다.  

이전 Pixel들을 정의하는데 RNN을 사용하는 방법이 바로 **PixelRNN**이다.  

인접한 픽셀들을 이용하여 순차적으로 픽셀을 생성한다.  
RNN 모델 중 LSTM을 이용하여 모델링을 한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92320930-52820200-f060-11ea-848d-9488a61ad0cf.png" width = "500" ></p>

하지만 순차적으로 픽셀들을 생성하기 때문에  
굉장히 속도가 느리다는 것이 단점이다.  

이러한 단점을 보완하기 위해 나온 방법이 **PixelCNN**이다.  

RNN 대신 CNN을 이용하여 픽셀을 생성하는데,  
CNN을 이용하면 특정 영역만 사용하기 때문에 훨씬 더 빠르게 이미지를 생성할 수 있다. (병렬적으로 Training도 가능하다!) 

PixelRNN과 PixelCNN 모두 Training은 Training Image에 대해 Likelihood를 최대로 하도록 training을 시킨다.  

Likelihood란 확률 분포의 모수가, 어떤 확률변수의 표집값과 일관되는 정도를 나타내는 값이다. (by [위키백과](https://ko.wikipedia.org/wiki/%EA%B0%80%EB%8A%A5%EB%8F%84))

PixelRNN과 PixelCNN을 요약한 슬라이드는 다음과 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92321092-1b145500-f062-11ea-9596-72387ccc70e3.png" width = "500" ></p>

### Variational Autoencoders

앞서 살펴본 PixelCNN 같은 경우 확률 모델이 계산 가능한 함수로 정의가 되었다.  

하지만 VAE(Variational Autoencoders)는 확률 모델이 계산 불가능한 함수로 정의가 된다.  

따라서 Lower bound(하한선)를 구해서 계산 가능한 형태로 만들어주자!  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92321226-451a4700-f063-11ea-9a44-d5441e25333e.png" width = "500" ></p>

먼저 VAE(Variational Autoencoders)를 배우기 전에  
Autoencoder의 과정, Encoder와 Decoder가 무엇인지 한 번 알아보자.  

Autoencoder는 Deep Neural network를 이용하여 데이터의 추상화를 위해 사용한다.  

기존의 차원축소 문제들은 Unsupervised Learning이었는데 이를 encoder와 decoder로 이루어져있는 NN를 이용하여 supervised Learning문제로 바꾸어서 해결하였다.  

데이터를 압축해서 다시 복원을 해주는데 training data에 대해 복원을 잘 해주는 방식으로 차원 축소를 하기 위해 사용하는 방식이 AutoEncoder이다.  

AutoEncoder이란 개념을 처음 들어봐서 간략하게 정리를 했는데  
[AutoEncoder (self-supervised learning) [설명/요약/정리]](https://lv99.tistory.com/22)에 좀 더 자세한 설명이 나와있다.  

**Encoder**란 <u>Input data X가 들어왔을 때,  
그 특징 벡터 z를 추출하는 과정</u>을 말한다.  

Encoduer의 예로는  
> - Sigmoid
> - Fully Connected Layer
> - ReLU, CNN

등이 있다.  

특징 벡터 z는 보통 Input data X보다 작다.  
즉, 차원이 줄어든 형태이다.  

그 이유는 Input data에서 중요한 특징들만 담고있는 특징 벡터를 이용하여 z를 구성하고 싶기 때문이다.  

다시 말해 Noise라고 생각되는 부분을 제거하고 싶기 때문이다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92321368-5a43a580-f064-11ea-9304-0ba3ace9a076.png" width = "500" ></p>

그렇다면 어떻게 Feature Representation을 학습할 수 있을까?  

바로 원본을 다시 복원(reconstruct)하는데 사용할 수 있는 특징들을 학습하는 방식을 취한다.  

아까 Encoder를 설명했으니 이제 Decoder가 무엇인지 알아보자.  

**Decoder**란 <u>특징 벡터 z를 통해  
Input data처럼 재구성하는 과정</u>을 말한다.

쉽게 말해 Decoder는 Encoder의 역과정이다.  

Decoder의 예도 Encoder과 비슷하다. (하지만 역의 과정을 거친다.)  

> - Sigmoid
> - Fully Connected Layer
> - ReLU, CNN(upconv)  

복원된 이미지와 원본 이미지의 차이를 계산하기 위해서  
L2 loss function을 사용한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92321625-5c0e6880-f066-11ea-99f0-20d18a69a17d.png" width = "500" ></p>

Decoder는 training할 때만 쓰이고  
실제로 test를 할 때는 Encoder만 가지고 진행한다.  

Encoder는 초기에 supervised learning에 의해 초기화 된 모델의 초기값을 이용한다.  

즉 Unsupervised learning을 self-supervised learning 문제로 풀 수 있도록 바꾼 것이 AutoEncoder라고 할 수 있다.  

VAE(Variational Autoencoders)는 AutoEncoder가 중요한 특징 벡터들을 잘 추출하고 있는데 이러한 벡터들을 이용하여 이미지를 재구성할 수는 없을까라는 아이디어에서 나온 방법이다.  

VAE를 통해 이미지를 재구성하는 방법에 대해 알아보자!  

먼저 학습 데이터가 있는데 이 학습데이터가 관측할 수 없는 잠재 변수(특징 벡터) z에 대해서 재구성 되는 것이라 가정하자.  

z는 다양한 종류의 특성들을 담고 있는데  
예를 들어 얼굴을 생성할 때 특성을 담고 있다고 하면,  
입꼬리가 올라갔는지, 눈썹의 방향, 머리 스타일 등을 담고 있다고 생각하면 된다.  

강의에서 prior라는 용어가 자주 나오는데,  
구글링을 통해 알아본 결과,  
Prior는 <u>확률 분포 관점에서 어떠한 event가 일어날 지에 대한 기대값</u>이라고 한다.

보통 prior를 선택할 때 가우시안 분포를 사용하여 표현한다. 

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92322210-da6d0980-f06a-11ea-85dd-c9ec2659f93a.png" width = "500" ></p>

생성 모델을 잘 만들기 위해서는 $\theta ^*$의 파라미터 값들을 잘 추정해야 하는데,  

여기서 나온 파라미터는 2가지가 있다.  

> - Prior distribution
> - Conditional distribution

Prior distribution의 같은 경우 가우시안 함수로 간단하게 표현하지만,  
Conditional distribution의 같은 복잡한 함수의 경우 Neural Network를 이용하여 표현한다.  

그렇다면 VAE model은 어떻게 training을 시킬까?  

우선 $P_{\theta}$를 정의한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92323122-11461e00-f071-11ea-9dd2-65ca8204d090.png" width = "300" ></p>  


하지만 여기서 우리가 간과한 사실이 있다..  
바로 $P_{\theta}$ 는 계산이 불가능한 녀석이라는 것이다. 계산이 불가능한 녀석인 이유는 $P_{\theta}(x|z)$ 가 모든 z에 대해서 알아내야 $P_{\theta}$ 를 구할 수 있는데 모든 z는 우리가 알아낼 수 없는 녀석이기 때문이다! 따라서 추가적인 encoder를 통해서 학습을 시키는데 $P_{\theta}(x|z)$를 근사한  
$Q_{\theta}(x|z)$ 를 이용하여 학습을 시킨다. 이 때 $Q_{\theta}(x|z)$ 는 계산 가능한 녀석이여야 한다.  

VAE의 encoder와 decoder에 대해서 한 번 알아보자.  

이 역시 이해가 좀 부족한 상태에서 포스팅을 했다.  

자세한 내용은 실제 논문을 참고해야 할 것 같다.  

AE(AutoEncoder)와 유사한 구조이지만 여기에 확률론적 의미가 추가된다.  

Encoder와 Decoder에 대한 Network는 다음과 같다.  
이 부분은 사실 이해가 제대로 되지 않았다. :(
자세한 내용은 강의 33분 28초를 참고하자.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92323308-9e3da700-f072-11ea-943c-9c3cd6bb57d2.png" width = "500" ></p>

이제 Data likelihood 관점으로 다시 돌아오면,  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92324647-b5829180-f07e-11ea-8f5c-cc8edf95ea45.png" width = "500" ></p>

다음과 같은 어마 무시한 식들을 볼 수 있다..  

$logp_{\theta} {x^{(i)}}$에 대한 식을 나타냈는데,  
기댓값으로 표현하고 log의 성질에 의해 다음과 같은 3항으로 나눌 수 있다.  

여기서 문제가 되는 항이 바로 3번째 항인데 그 이유는 계산이 불가능하기 때문이다.  

하지만 정의에 의해 3번째 항은 0보다 크므로 다음 슬라이드와 같이 바꾸어 줄 수 있다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92324720-4bb6b780-f07f-11ea-8b3f-8b9e352d6adc.png" width = "500" ></p>
<br>

따라서 앞에 2항을 가지고 Training을 진행하자.  

Training의 과정은 다음과 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92324769-9f290580-f07f-11ea-8a34-11c838b3587f.png" width = "500" ></p>

<br>

Test를 할 때는 Decoder network를 이용하여 test를 한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92324860-a13f9400-f080-11ea-8fb3-c04ea53aebe9.png" width = "500" ></p>

서로 독립되어 있는 잠재 변수 z를 통해서 여러가지 이미지들을 생성할 수 있다.  

하지만 생성된 이미지들이 blur한 특징을 갖는 단점이 있다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92324887-e4016c00-f080-11ea-9300-5933552cce57.png" width = "500" ></p>

VAE는 <u>계산할 수 없는 형태의 확률 분포를 이용하여 계산 할 수 있는 형태로 근사 시켜 모델을 만든다는 특징</u>이 있다.  

하지만 <u>직접 최적화를 진행하는 PixelRNN/CNN보다 만들어진 이미지의 Quality가 떨어진다는 단점</u>이 있다. 

### GAN(Generative Adversarial Networks)

GAN은 앞서 살펴본 방법들과는 달리 확률 분포를 계산하는 것을 포기하고 단지 Sample을 얻어내는 방법을 말한다.  

즉, 결과만 뽑아내는 것이다.  

GAN을 구현하기 위해서는 두 가지의 Network가 필요하다.  

> - Generator Network
> - Discriminator Network

**Generator Network**는 Discriminator를 속여 실제처럼 보이는 가짜 이미지를 만들어 내는 것이 목적이다. 

**Discriminator Network**는 진짜 이미지와 가짜 이미지를 구별해 내는 것이 목적이다.  

GAN을 쉽게 이해하는데 흔히 '경찰'과 '도둑'을 예로 들어 설명하곤 한다.  

그에 대한 설명은 [새로운 인공지능 기술 GAN ② - GAN의 개념과 이해](https://www.samsungsds.com/global/ko/support/insights/Generative-adversarial-network-AI-2.html)를 참고하면 도움이 될 것이다.  

이 강의에서는 다음과 같이 GAN을 설명했다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92325021-26777880-f082-11ea-9688-c7beb51bb0c0.png" width = "500" ></p>

GAN의 objective funtion은 다음과 같다.  


<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92325037-4e66dc00-f082-11ea-9a00-18c3f0a19183.png" width = "500" ></p>

Discriminator network와 Generator Network를 번갈아 가면서 학습을 진행한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92325065-9554d180-f082-11ea-97f6-7ca3bad003f1.png" width = "500" ></p>

실제로는 Generator Network의 학습이 제대로 이루어지지 않는 것을 확인할 수 있는데, Gradient의 정도가 오차가 심할 때 크지 않기 때문이다.  

즉, Loss값이 큰 경우 기울기 값이 크게 되어 update가 매끄럽게 진행되어야 하는데,  
그 반대로 Loss값이 작은 경우 기울기 값이 크기 때문에 약간의 Trick을 이용하여 Objective Function을 바꾸어준다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92325135-2a57ca80-f083-11ea-976f-e77438eb7ae2.png" width = "500" ></p>

다른 Network들이 Training을 할 때 batchsize를 정해주듯이,  
여기서는 k의 값으로 한 번에 training을 할 양을 정해준다.  

Training을 완료한 후,  

Test를 할 때는 Generator Network를 이용하여 Test를 진행한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92325212-b7028880-f083-11ea-8bf8-49049141ccac.png" width = "500" ></p>

GAN을 통해 만든 결과물이다.  

---

GAN을 이용한 연구는 정말 무궁무진하게 쏟아지고 있는데,  
그 중 하나가 CNN과 GAN을 결합하는 방법이다.  

이 방법은 **DCGAN**이라고 부른다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92325241-fb8e2400-f083-11ea-9eb5-4904c611caea.png" width = "500" ></p>

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92325259-12cd1180-f084-11ea-94af-833aae08bcfc.png" width = "500" ></p>

이 방법을 이용하면 그냥 GAN을 이용한 것 보다 훨씬 더 진짜같은 이미지들을 많이 만들어 낼 수 있다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92325284-3d1ecf00-f084-11ea-8d59-f536b0c9a1f7.png" width = "500" ></p>

또 다른 방법으로는 해석 가능한 벡터들을 가지고 연산을 수행하여 새로운 이미지를 만들어 내는 방법이다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92325306-6f303100-f084-11ea-9472-5ee5a1ae5804.png" width = "500" ></p>

정말 신기하지 않은가...

GAN에 관한 연구들을 모아논 github도 있다.  
[The GAN ZOO](https://github.com/hindupuravinash/the-gan-zoo)에 들어가면 정말 무수히 많은 논문들이 존재한다.  

---

오늘 배운 세가지 생성 모델을 정리하면 다음과 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/92325378-04332a00-f085-11ea-84e5-6eb9a693c495.png" width = "500" ></p>

최근에는 이 3가지 모델을 결합하는 시도도 있다고 한다.  

이번 강의는 확률에 대한 이야기가 많이 언급되었는데,  
확률도 차차 공부를 시작해야겠다.  

















































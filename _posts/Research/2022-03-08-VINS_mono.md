---
title : "[Paper Review] VINS-mono 요약 및 설명"
category :
    - Research
tag :
    - Paper_Review  
    - VINS
toc : true
toc_sticky: true
comments: true
---  

Visual Inertial SLAM 중 유명한 VINS-mono를 요약해보자. [Work in Progress...]  

최근 VIO, VI-SLAM 쪽 연구를 진행하려고 하고 유명한 논문을 먼저 읽고 공부해 보고자 한다.  
Visual Inertial쪽에서 제일 인용이 많이 된 논문 중 하나인 VINS-mono를 읽어보자.  

논문은 아래 링크에서 원본을 확인할 수 있다.  

- VINS-Mono: A Robust and Versatile Monocular Visual-Inertial State Estimator ([ArXiv](https://arxiv.org/pdf/1708.03852.pdf) , [IEEE](https://ieeexplore.ieee.org/document/8421746))      
- Github : [HKUST-Aerial-Robotics/VINS-Mono](https://github.com/HKUST-Aerial-Robotics/VINS-Mono)  

## 1. Introduction  

VINS-mono는 Monocular camera와 IMU를 결합했을 때 단순히 카메라를 썻을 때보다,   여러 장점이 있다고 소개한다.  

- Scale 값을 얻을 수 있다. (Roll, pitch angle도 얻을 수 있다.)  
- 카메라쪽에서 Tracking하기 어려운 환경(illumination change, textureless area, or motion blur)에서도 IMU 센서의 도움을 받아 Tracking을 수행할 수 있다.  
- Camera, IMU 모두 저가로 구입할 수 있는 센서이다.  

이러한 장점이 있는 반면 큰 Issue들도 존재한다.  

- 초기화 과정이 힘들다.  
> 직접 거리를 측정할 수 있는 센서가 없기 때문에, visual structure 와 inertial measurements의 결합이 어렵다.  

- VINS(visual-inertial system)은 Non-linear한 System이다.  
> 대부분의 경우 시스템은 고정 위치에서 시작해야 하며 처음에는 천천히 조심스럽게 움직여야 하므로 실제로 사용이 제한된다.  

- VIO(Visual-inertial odometry)의 경우, drift현상이 필연적이다.  

따라서 이런 Issue들을 해결하기 위해 VINS-mono에서는 다음과 같은 Contribution이 존재한다.  

- 초기 상태(initial states)를 모르더라도 초기화 할 수 있는 방법을 제안  
- Tightly coupled 방식의 sensor fusion, Optimization-based Estimation 방법  
- 실시간이 보장된 Relocalization and 4 degrees-of-freedom (DOF) global pose graph optimization  
- pose graph를 저장하고 불러오고 다른 local pose graph와 합칠 수 있음  

## 2. OverView  

Related work 부분은 생략을 할 예정이다.  

VINS-mono의 전반적인 system은 아래 그림과 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/157216142-b98824d8-56d8-41a5-afe7-9f2d0e7a5760.png" width = "800" ></p>  

VINS-mono는 우선 measurement preprocessing 과정을 거친다. 이미지로부터 feature를 뽑아내고, Tracking을 하면서 연속적인 두 이미지 사이에 IMU measurments를 preintegrated하는 과정을 거친다.  

Measurement preprocessing 과정이 끝나면 초기화(initialization) 과정을 거친다. 추후 nonliear optimization을 할 때 필요한 값들을 모두 구한다. 예를 들어, pose, velocity, gravity vector, gyroscope bias, and (3-D) 특징 점의 위치 등등을 구하게 된다.  

초기화 값으로 구한 값을 가지고 Relocalization Module이 있는 Visual-Inertial Odometry Module은 preintegrated 된 IMU 측정값과, feature observations를 tightly coupeld 방법으로 fusion을 진행한다.  

VINS-mono에서는 Monocular camera와 IMU값을 이용하여 초기화하는 방법, Keyframe을 고르는 방법, tracking을 잘 수행하는 방법등을 제안했으며, Loop closure과 pose graph reuse 모듈까지 만들어 전체적인 SLAM system을 구축했다.  

이 논문에서 사용하는 Notation은 다음과 같다.  
- $()^c$ : camera frame  
- $()^w$ : world frame  
- $()^b$ : IMU frame  
- $q$ : 쿼터니언, $R$ : Rotation Matirx  
- $p$ : Translation vector  
- $b_k$ : k번째 이미지에서의 IMU(Body) frame  
- $c_k$ : k번째 이미지에서의 Camera frame  
- $\otimes$ : 쿼터니언끼리의 곱  
- $\hat{()}$ : 추정 값 or noisy measurement  

## 3. Measurement preprocessing  

Visual measurment와 IMU measurement의 전처리 과정을 자세하게 살펴보자.  

Visual measurment에서는 연속적인 이미지에서의 Tracking을 시도하고, 현재 frame에서 새로운 특징점을 찾는다. IMU measurements에서는 연속적인 두 이미지에서 preintegration과정을 거친다.  

### Vision Processing Front End  

새로운 이미지가 들어오면, KLT sparse optical flow algorithm을 수행한다. 한 이미지당 featrue의 갯수는 100개~300개 정도를 유지한다. Detector는 feature간 너무 붙어 있지 않도록 uniform feature distribution를 적용한다.  

RANSAC 알고리즘을 이용하여 Oulier 제거를 거친 후, 특징이 추출된 이미지를 unit sphere(단위 구)에 투영을 시킨다.  

이 과정에서 KeyFrame 선별도 하게 되는데 두 가지 기준을 가지고 KeyFrame을 선별한다.  

- last(최근) frame과 current(현재) frame간에 특징점들 끼리의 픽셀 차이(parallax)가 일정 threshold 이상일 경우 새로운 keyframe으로 구분  
- Tracking quality에 따라 구분  

특징점 끼리의 픽셀 차이를 여기서 parallax(시차)라고 표현했다. 이러한 기준을 선정한 이유는 triangulate를 진행할 때 충분한 feature 수를 확보하기 위해서이다. 또한 parallax를 구할 때 Rotation만 일어났을 때만을 대비하여 IMU measurement에서 측정된 short-term integration of gyroscope measurements를 보상하여 parallax를 계산한다.   

Tracking quality는 이미지에서 tracked features의 수로 판별한다. 일정 Threshold를 넘길 경우, Good track으로 판별하여 새로운 KeyFrame으로 선별한다.  

### IMU pre-integration  

IMU pre-integration이란 연속적인 카메라 이미지 데이터에 IMU 정보를 결합하여 더 정교한 pose를 계산하기 위한 작업이다. 보통 아래의 그림처럼 IMU가 들어오는 간격이 카메라 이미지보다 더 빠르기 때문에 카메라 이미지 사이에 들어오는 IMU measurement들을 하나로 이미지와 싱크를 맞추는 작업을 거친다.   

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/157402471-f928fb47-fe89-4586-b8fa-d1169aa14605.png" width = "500" ></p>  

IMU가 어떻게 동작하는지 우선 이해가 필요하다. 나도 이번 기회를 통해서 조금 깊게 파보려고 공부중인데 다음과 같은 링크가 도움이 되었다.  
- [[IMU] IMU의 개념 및 활용법](https://velog.io/@717lumos/Sensor-IMU%EC%9D%98-%EA%B0%9C%EB%85%90-%EB%B0%8F-%ED%99%9C%EC%9A%A9%EB%B2%95#1-%EA%B0%80%EC%86%8D%EB%8F%84-%EC%84%BC%EC%84%9C)  
- [INERTIAL NAVIGATION PRIMER](https://www.vectornav.com/resources/inertial-navigation-primer)  

기본적으로 IMU는 gyroscope(각속도)와 accelerometer(가속도)를 측정하는 센서이다. 각각의 값들은 Noise와 bias가 존재하고 VINS-mono에서는 IMU를 다음과 같이 모델링 했다.  

$a_t$는  linear acceleration 값을 나타내고, $w_t$는 angular velocity이다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/157408923-863160a2-79be-44f7-afb9-7ef1e25e8b02.png" width = "400" ></p>  

각각 노이즈는 Gaussian white noise를 가정한다. 그리고 Bias값도 미분을 하게 되면 Gaussian white noise이 나오도록 모델링했다.  

[k, k+1] image frame 사이에 IMU의 translation 값 $p$, velocity 값 $v$, quaternion 값 $q$는 아래와 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/157418277-2057319a-f1ea-49a8-bf3b-1e20e974393c.png" width = "500" ></p>  

여기서 $g^w$는 world 좌표계에서의 중력 값이 아닌, IMU body frame으로 투영 된 중력 값이다.  

여기서 우리는 world 좌표계가 기준이 아닌 $b_k$(k번째 이미지에서의 IMU body frame)을 기준으로 계산을 해야하므로 좌표계 변환이 필요하다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/158013605-5c51346c-1a3c-4b41-b306-c6446cc9b3b8.png" width = "500" ></p>  

따라서 결론적으로 구해야 하는 IMU의 translation 값 $$p = \alpha^{b_k}_{b_{k+1}}$$  

IMU의 velocity 값 $$v = \beta^{b_k}_{b_{k+1}}$$  

IMU의 quaternion 값 $$q = \gamma^{b_k}_{b_{k+1}}$$  

이라고 하면 아래와 같이 정리할 수 있다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/157420920-84ac9d3a-4801-4801-b12e-c2e469a8ba73.png" width = "350" ></p>  

위 수식에 대한 정리는 [Formula Derivation and Analysis of the VINS-Mono](https://arxiv.org/abs/1912.11986)에서 IMU PREINTEGRATION 파트를 보면 자세히 나와있다.  

컴퓨터는 discrite time을 가지고 있기 때문에 위에 나온 preintegration term을 discrite time에서 나타내면 식은 아래와 같다. 이때 우리는 Mid point method를 사용한다.   

$i$는 $[t_k, t_{K+1}]$에서 discrete moment를 나타낸다.  

$\delta_t$는 $i,i+1$에서의 time interval이다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/158013233-7f211e39-b41b-4838-bf88-f2164d366425.png" width = "550" ></p>  

[... 하아 일단 수식이 너무 어려움으로 나중에 다시 봐야겠다...]  

논문에 의하면 $k$번째 Image에서의 IMU measurement를 $b_k$라고 하고 $k+1$번째 Image에서의 IMU measurment를 $b_{k+1}$이라고 할 때, 그들의 관계를 나타내는 covariance $P$는 아래와 같이 나타낼 수 있다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/158014138-21ec4772-f400-4411-814f-768efb126046.png" width = "500" ></p>  

## 4. Estimator Initialization

VINS(visual-inertial system)에서 초기화는 굉장히 어려운 Issue이다. 카메라(이미지)와 IMU 센서를 tightly coupled로 결합 방식을 택한다면 더욱 더 복잡한 non-linear system이 된다.  

따라서 VINS-mono에서는 초기화 과정을 크게 두 가지 과정으로 거쳐 문제를 풀려고 한다.  

1. Visual only SFM(Structure from Motion)  
2. Visual-Inertial Alignment  

먼저 Visual 정보만으로 대략적인 초기 값을 구한 다음, IMU measurement 값을 algin(정렬)시키는 방법이다.  

논문에서는 아래의 그림으로 표현하고 있다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/158017879-ac1b653c-f5df-4c0a-88d4-abde345ee334.png" width = "600" ></p>  

Visual only SFM의 경우 monocular camera이기 때문에 scale을 모르기 때문에 IMU pre-integration 부분과 차이가 날 수 있다.  

하지만 처음부터 Visual과 IMU가 Aligned된 초기값을 구하는 것이 어려움으로 이렇게 2 step으로 나누어 진행한 것으로 생각한다.  

또한 특이하게도 Initialization을 할 때는  accelerometer bias term을 무시하고 초기화를 진행한다. accelerometer bias term의 경우 중력의 영향을 받기 때문에 초기화 단계에서는 정확하게 accelerometer bias term을 구하기 힘들기 때문이다.  

### Visual only SFM  

Visual only SFM으로 scale을 제외한 camera pose와 3D feature position을 구하게 된다.  

논문에서는 "Sliding Window Vision-Only SfM"이라는 Part에 나와있는데 Sliding window란 N개의 keyframe 정보를 기반으로 SfM을 수행한다고 이해하면 될 것 같다. VINS-mono를 설명하는 [Monocular Visual-Inertial SLAM - Shaojie Shen](http://ice.dlut.edu.cn/valse2018/ppt/Monocular_Visual-Inertial_SLAM_SJShen.pdf)의 자료에 따르면 1초당 10frame 정도 Widnow를 설정한다고 했다.   

Five point algorithm으로 두 이미지 간에 Rotation과 Translation의 값을 구한 다음, 임의의 scale 값을 추가하여 Triangulation을 수행한다.(Monocular camera로는 아직 정확한 scale값을 모르기 때문이다.)  

Triangulation은 PnP 알고리즘을 활용하며 Window 안에 있는(N개의 keframe) 추정된 모든 camera pose에 대해서 수행하게 된다.  

그 다음 Window에 있는 모든 estimated camera pose와 3D feature points에 대해서 Bundle adjustment를 수행한다.  

아직 World frame에 대한 정보가 없으므로 첫번째 camera frame($()^{c0}$)를 기준으로 위 과정을 수행하며 Camera와 IMU간의 extrinsic parameter를 알고 있는 경우에 좌표계 변환을 이용하여 camera frame $c0$ 부터 $k$번째 IMU body frame $b_k$의 quternion $q$와 Translation $p$를 계산한다. 

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/158019482-bc52d182-9493-44b9-a6cf-c21dd93766b5.png" width = "300" ></p>  

위 식에서 $s$는 scale factor로 아직까지는 unknown parameter로 Visual-Inertial Alignment를 수행하며 값을 계산을 하게 된다.  

### Visual-Inertial Alignment  

Visual only SFM으로 카메라간의 Roation과 Translation을 구했다면 이제 IMU preintegration 값과 align을 통해 scale factor $s$와 VIO initialization을 진행해보자.  

첫번째로 Gyroscope Bias calibration을 진행한다. Visual only SFM 과정으로 $$q^{c0}_{b_{k+1}}, q^{c0}_{b_{k}}$$   

또한 , Preintegration 과정에서 $$ \gamma^{b_k}_{b_{k+1}} $$ 을 구했다.  

이론상으로는 Visual only SFM 과정으로 구한 쿼터니언 값과 Preintegration 과정에서 구한 쿼터니언 값을 쿼터니언 곱셈을 하게 되면 $(qx, qy, qz, qw) = (0, 0, 0, 1)$ 값이 나와야 한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/158556923-ec972fcb-c73c-48da-8757-80524d3becfb.png" width = "250" ></p>  

앞서 말했지만 IMU의 quaternion 값 $$q = \gamma^{b_k}_{b_{k+1}}$$  

이렇게 정의했었다. 

하지만 real world에서는 noise 및 bias 값이 있기 때문에 위 식이 $(qx, qy, qz, qw) = (0, 0, 0, 1)$ 값이 아닌 다른 값이 나오게 되고 이를 최소화를 시켜줘야 정교한 값을 구할 수 있게된다.  

이 때 위 식에 영향을 미치는 Term이 바로 Gyroscope Bias($b_w$)이다. 왜냐하면 Preintegration 과정에서 IMU의 quaternion 값을 선형화를 진행하는데 그 때 아래의 식을 사용하기 때문이다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/158558288-dd3cc94f-56bc-4d38-8376-a08d4a2c7131.png" width = "300" ></p>  

따라서 아래의 식을 통해 Gyroscope Bias($b_w$)을 최소화한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/158558588-7ed37fb5-4444-4ef2-859f-3db24297a465.png" width = "350" ></p>  

그 다음 새로 구한 Gyroscope Bias($b_w$)을 이용해 IMU pre-integration terms를 다시 계산한다. (논문에서는 re-propagate라고 표현)  

그 다음 아래의 state vector를 초기화를 진행하게 된다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/158558932-6a640ca6-265f-41c1-94f0-1ed268dc905a.png" width = "450" ></p>  

IMU body frame $[b_0, b_n]$까지의 velocity 값과 첫번째 camera frame $c_0$에서의 gravity 값, 그리고 스케일 값 $s$로 이루어진 vector이다.  

state vector는 Window안에 있는 linear measurement model을 통해서 최적화가 이루어지게 된다.  

IMU preintegration에서 구한 값을 활용하여 world frame에서 첫번째 camera frame으로 좌표계 변환을 해주면 아래의 식이 나오게 된다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/158561660-d13a3302-946a-491e-b44e-3e64150a662c.png" width = "450" ></p>  

선형화를 했으니 Matrix decomposition이 가능하다 따라서 아래와 같이 행렬화를 시켜줄 수 있다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/158562099-99c41fd1-fb5b-4378-b072-f2985badde0f.png" width = "550" ></p>  

논문에서는 위에 있는 식에있는 가운데 있는 Matrix를 아래(식.11)처럼 $H$로 정의했다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/158563094-08dac468-11f1-43f5-a441-8cce465c5d0d.png" width = "550" ></p>  

따라서 IMU sensor 를 통해 linear acceleration을 측정할 수 있고 측정한 것($\hat{z}$)을 아래와 같이 표현할 수 있다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/158564562-bf33ebe3-99b8-4d11-ad1a-081d64d1ca92.png" width = "600" ></p>  

이상(Ideal)한 값과 측정(Measurement) 값에는 오차가 발생함으로 이를 Least squares method로 최적의 해를 찾을 수 있다. 여기서 찾는 최적의 해는 state vector이다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/158565092-99dd0b2c-6853-4299-b8ba-8ee6decf02fc.png" width = "300" ></p>  


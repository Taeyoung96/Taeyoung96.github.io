---
title : "[Paper Review] LVI-SAM 요약 및 설명"
category :
    - Research
tag :
    - Paper_Review  
    - LVI
toc : true
toc_sticky: true
comments: true
---  

LiDAR-Visual-inertial SLAM 논문 중 하나인 LVI-SAM을 읽고 요약해보자.  

논문은 아래 링크를 통해 확인할 수 있다.  
코드도 공개되어 있다.  

- LVI-SAM : Tightly-coupled Lidar-Visual-Inertial Odometry via Smoothing and Mapping  
    ([ICRA 2021](https://ieeexplore.ieee.org/abstract/document/9561996), [Arxiv](https://arxiv.org/abs/2104.10831))  
- Github : [TixiaoShan/LVI-SAM](https://github.com/TixiaoShan/LVI-SAM)  

LVI-SAM의 주요 Contribution은 다음과 같다.  
- LiDAR-Camera-IMU를 tightly coupled 방식의 sensor fusion을 사용하여  
  state estimation 및 Mapping을 진행  
- VIS(Visual-inertial-System)의 state가 LIS(LiDAR-inertial-System)의 초기화에 도움을 주도록 설계  
- LiDAR point cloud를 사용하여 3D visual feature point의 depth information을 추출  
- VIS(Visual-inertial System)이 실패하더라도 LIS(LiDAR-inertial System)이 도움을 주고, 그 역도 마찬가지  

참고로 LVI-SAM의 1저자인 Tixiao Shan은 LIO-SAM (IROS 2020)과 LeGO-LOAM (IROS 2018)이라는 유명한 LiDAR SLAM을 만드신 분이다. LVI-SAM은 LIO-SAM이라는 LiDAR-inertal SLAM에 VINS-MONO (TRO 2018)을 결합한 것이라 생각하면 이해하기 쉽다. 따라서 두 논문을 모두 이해하고 있다면 LVI-SAM을 비교적 수월하게 읽을 수 있다!  

VINS-MONO에 대한 요약 포스팅은 [[Paper Review] VINS-mono 요약 및 설명](https://taeyoung96.github.io/research/VINS_mono/)에서 확인할 수 있다.  

## 1. Introduction  

Introduction에서는 Visual-inertial System과 LiDAR-inertial System에 대한 소개, 그리고 LVIO(LiDAR-Visual-inertial odometry)를 사용했을 때의 장점에 대해서 설명해주고 있다. 각각 센서마다 장,단점을 가지고 있고 센서별로 잘되는 환경이 각각 다른데, 3개의 센서를 융합하여 센서간의 단점을 보완하고 여러 환경에서도 강인한 SLAM 알고리즘을 만들 수 있다는 것이 LVIO의 가장 큰 장점이다.  

수 많은 VIS 알고리즘 중에서 VINS-MONO framework를 LVI-SAM에 채택한 이유는 Monocular Visual-inertial System에서 가장 SOTA의 정확도를 보이고 있기 때문이라 설명한다. 그리고 LIS 알고리즘 중에서 LIO-SAM을 선택한 이유는 아까도 언급했지만, LVI-SAM 저자 본인의 작품(?)이기 때문이다. (논문에 그렇게 나와있지 않지만 그런 생각이 든다.)  

LVI-SAM에서 주장하는 Contribution을 다시한번 정리해보면 다음과 같다.  

1. Graph based tightly-coupled LVIO(LiDAR-visual-inertial odometry) framework를 제안  
2. Failure detection을 통해 sub-system들을 각각 통제할 수 있도록 알고리즘을 제안  
3. 여러 환경, robot platform에서 실험하여 Robustness를 입증함.  

## 2. Overview  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/187835386-4a08be2c-3996-4090-bb1d-420726b5d7d9.png" width = "900" ></p>  

LVI-SAM은 크게 두 가지 sub-system으로 이루어져 있는데, 하나는 VIS(Visual-inertial System), 그리고 또 다른 하나는 LIS(LiDAR-inertial System)이다. VIS는 대부분 VINS-MONO에서 코드를 활용했으며, Original VINS-MONO와 달라진 점은, 3D visual feature points의 depth를 LiDAR point cloud를 이용하여 구한다는 것이다. LIS는 LIO-SAM에서 코드를 활용했다.  

논문과 코드를 비교해보면, 논문에 있던 것과 조금 다르게 구현을 했다는 것을 알 수 있다. 논문에서는 IMU preintegration constraints, Visual odometry constraints, LiDAR odometry constraints, 그리고 Loop closure constraints를 동시에 Graph Optimization을 수행하는 것처럼 설명이 나와있는데, 실제 코드를 들여다보면 LiDAR odometry와 IMU만을 사용하여 Graph Optimization을 수행한다.  

이는 [Github issue](https://github.com/TixiaoShan/LVI-SAM/issues/7) 상에서도 열띤 토론을 했고, 여러 사람들이 논문과 코드가 다르다고 느끼는 부분이였다.  

처음 구현은 모든 constraints를 넣고 최적화를 시도했지만 실제로 VIS(Visual-inertial System)이 LIS(LiDAR-inertial System)에 별로 도움을 주지 못하여 제거를 한 것이 아닌가하는 생각이 든다.  


## 3. Visual-inertial System  

기존의 VINS-MONO와 차이가 나는 부분에 대해서만 논문에서 언급을 하고 있다.  

기존의 VIO의 초기화가 어렵다는 것을 설명하고 있는데, 그 이유는 monocular camera 특성상 scale을 추정하는데 IMU에서 충분한 움직임(acceleration excitation)이 있어야 하기 때문이다. 따라서 scale을 추정하는 것을 LiDAR point cloud의 depth association을 통해서 해결했다는 것이 LVI-SAM의 contribution이다.  

VIS의 state의 초기화를 LIS의 state 및 bias를 통해서 진행하므로써 조금 더 빠르고 정확한 초기화가 가능하다.

3D visual feature points를 LiDAR point cloud와 association하기 위한 방법론을 제시를 한다. 우선 Visual feature points와 LiDAR point cloud를 모두 원점이 카메라 중심인 단위 구(a unit sphere)에 투영을 시킨다. 그리고 polar coordinate를 기준으로 2차원 K-D tree를 이용하여 3D visual feature points와 가까운 3개의 LiDAR point cloud 점을 뽑게 된다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/188349206-a40f23f8-8df1-492a-897a-7c3bb7cd2f5e.png" width = "500" ></p>  

LiDAR depth points의 경우, LiDAR frame을 쌓아서 사용을 하기 때문에 서로 다른 object에 대해 depth ambiguity가 발생한다. 이러한 경우 가까운 depth points에 대해 association을 진행하는 과정을 거친다.  

아래 그림과 같이 Camera pose가 $t_i$에서 $t_j$로 옮겨지는 경우가 있는데, 이 때 회색으로 된 point가 $t_j$에서만 관찰되는 점이다. 초록색 점이 회색 점선으로 되어 있는 point들에 대해 어떤 LiDAR point를 선택해야 하는지 depth ambiguity가 발생하는데, 이 때 가까운 거리에 있는 depth point를 선택하는 전략이다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/188371272-bb3729eb-c3f3-4f15-9851-68f7cb77c07b.png" width = "400" ></p>  
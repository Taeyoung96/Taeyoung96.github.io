---
title : "Slam 4-4강 (Robust Least Squares for Graph-Based SLAM) 요약"
category :
    - Slam
tag :
    - Slam
toc : true
toc_sticky: true
comments: true
---  

Cyrill 교수님의 SLAM 강의를 듣고 정리해보자!  

## 0. Today's Goal  

- Least Squares approach의 단점  
- MaxMixtures or Dealing with Multiple Modes  
- Dynamic Covariance Scaling  
- Least Squares with Robust Kernels

with Robust Kernels


## 1. 강의 및 강의 자료  

강의 List는 [SLAM KR](https://www.youtube.com/channel/UCXvT7auo7xUd7v0B2pmvwIA)의 SLAM DUNK seson 2에서 정해준 강의를 따라 정리를 할 것이다.  

SLAM DUNK season 2의 Reference는 [여기](https://youtube.com/playlist?list=PLubUquiqNQdMYwQVftUSFEWhJgzBErO9N)서 확인해 볼 수 있다.  

SLAM KR에서 이 강의를 듣고 요약을 세미나 형식으로 준비를 해준다!  

이번 포스팅은 다음의 강의들을 참고하여 요약을 해보았다.  

- [Robust Least Squares for Graph-Based SLAM (Cyrill Stachniss)](https://youtu.be/z60RbiY18I8)  
- [SLAM DUNK Season 2 - Robust Least Squares for Graph-Based SLAM](https://youtu.be/BOqB4V18rPw)  

Cyrill Stachniss의 강의 자료는 pdf로 [다운](https://drive.google.com/file/d/1eID1m3DBoZAl3oOTWRBosry3I8hBoQyD/view?usp=sharing) 받을 수 있다.  

## 2. Least Squares approach의 단점  

이번 강의를 이해하기 위해선 [SLAM 4-1강 (Graph-based SLAM using Pose Graphs)](https://taeyoung96.github.io/slam/SLAM_04_1/)에서 배운 Graph-based SLAM에 대해서 이해를 하고 있어야 한다.  

우리가 지금까지 배웠던 Least squares 방법은 error의 제곱의 합에 해당하는 값을 최소화시키는 방법이다.  

이러한 방법에는 크게 두 가지 단점이 존재한다.  

1. Outlier에 굉장히 취약하다는 것  
2. Edge로 만드는 constraints가 Gaussian distribution을 따른다고 가정한다는 것  

센서 모델의 경우 이러한 Gaussian distribution을 따르는 것이 나쁜 결과를 초래하진 않지만, 문제는 Data association이 애매모호한 경우, 이러한 Gaussian distribution은 현실 세계(real world)와는 다르기 때문에, 우리가 원치 않은 값으로 최적화가 이뤄질 수 있다.  

Data association이 모호하게 나타나는 경우는 다음과 같은 경우에서 빈번하게 나타난다.  

1. Landmark들이 모두 비슷하게 보일 때  
2. 장소가 굉장히 유사한 곳을 mapping할 때  
3. GPS의 신호를 활용하여 Data association을 만들 때  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/153808573-a09a61c2-e1c7-4084-a1cb-8b47581a2974.png" width = "600" ></p>  

위 사진은 Data association이 모호하게 나타나는 경우의 한 예이다. 로봇이 비슷한 모양의 Landmark(검은색 직육면체)로 둘러쌓여 있을 때, 로봇의 pose에 대한 확률이 여러 곳에서 높게 나타난 것을 볼 수 있다.  

따라서 이번 강의에서는 Data association이 모호할 때, 우리가 어떻게 Outlier를 처리하고 Robust한 Least squares approach를 적용할 것인지 다룰 것이다. 다양한 방법론에 대해 소개하는 식으로 강의가 진행된다.  

## 3. MaxMixtures or Dealing with Multiple Modes  

그 첫번째 방법은 바로 MaxMixtures or Dealing with Multiple Modes이다.  

간단히 이 방법에 대해서 설명하면 Model을 정의할 때 하나의 Gaussian Model로 정의하는 것이 아니라 여러 모드로 일반화할 수 있는 방법이다.  

우리가 앞서 배웠던 Pose graph에서 최적화할 부분을 Gaussian Model로 정의하고 수식으로 풀어쓰면 다음과 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/153809537-3b2b7c21-e81c-4f0b-b7c3-bd50012439d2.png" width = "450" ></p>  

우리가 최적화를 할 때 자주 보던 $e^T_{ij} \Omega_{ij} e_{ij}$ 값에 Gaussian distribution만 적용한 것이다. $\eta$는 Gaussian distribution 식을 만들 때 사용한 상수값을 나타낸 것이다.  

Error minimization을 진행할 때는 Log likelihood를 사용하기 때문에 식은 아래와 같이 변형된다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/153813060-9ee6ffa0-e862-49f5-9426-5f5f3a8847cf.png" width = "500" ></p>  

지수(exponential)꼴로 복잡하게 정리됐던 식이 원소들의 덧셈으로 나름 아름답게(?) 정리가 된다.  

**우리가 multiple mode로 나타낼 수 있는 첫번째 방법은 서로 다른 Gaussian distribution을 합하는 방법이다.**  

그럼 식은 아래와 같이 달라진다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/153812464-999c6173-1688-436a-9c08-56c70963c114.png" width = "550" ></p>  

위 식과의 차이점은 서로 다른 $k$개의 Gaussian distribution에 대해서 Weight인 $w_k$를 부여하고 모두 합쳐서 식을 정의했다는 것이다.  

마찬가지로 Error minimization을 진행할 때는 Log likelihood를 사용하기 때문에 식은 아래와 같이 변형된다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/153813273-c022a6ae-46aa-48c8-8097-a7b8718aa986.png" width = "630" ></p>  

$\Sigma_k$가 있기 때문에 식을 정리하기가 까다롭다.  

따라서 이 식을 그대로 사용하는 것이 아닌 대략적인 값을 활용하여 식을 재구성한다. 강의에서는 가장 중요한 mode의 값만 남겨두는 방식으로 식을 만든다고 이야기한다.  

$\Sigma_k$ 연산자를 사용하는 것이 아닌 $max_k$ 연산자를 사용하여  Log likelihood를 사용할 때 식을 정리하기 수월하게끔 한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/153813843-cf41c35f-96c6-45df-918a-f78389ddf60e.png" width = "630" ></p>  

우리가 $max$ 연산자를 사용하기 때문에 방법의 이름이 **"Max mixture"** 이다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/153814954-8832dd5f-729c-49f5-b184-4ec7e374e99b.png" width = "630" ></p>  

위 그림 중 왼쪽은 두 Gaussian distribution을 나타낸 것, 가운데는 $max$ 연산자를 사용했을 때 Gaussian distribution, 오른쪽은 $\Sigma$ 연산자를 사용했을 때 Gaussian distribution이다. 오차가 조금 존재하지만 식을 간단하게 할 수 있기 때문에 $max$ 연산자를 사용해서 Gaussian distribution를 만든다.  

따라서 Max mixture 식에서 Log Likelihood를 적용한 식은 아래와 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/153815467-a5210a60-3129-4378-a510-dec1da0f7849.png" width = "650" ></p>  

### Max mixture 방법의 장점

<u>**"Max mixture"** 을 사용하면 우리가 기존에 해왔던 Error minimization을 서로 다른 Gaussian distribution에 따라서만 값을 바꿔주고 최소값을 찾으면 되기 때문에 우리가 기존에 사용했던 Error minimization식을 그대로 활용할 수 있다는 장점</u>이 있다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/153816951-8f844543-1abd-480e-973f-27533f348bd9.png" width = "650" ></p>  

위 그림에서도 알 수 있듯이, 시간적인 측면에서도 기존의 Error minimization 방법과 차이가 얼마 나지 않는다는 것을 알 수 있다.  

또한 Outlier에 굉장히 강인하게 작동할 수 있다. 서로 다른 모드를 적용하여 하나는 주요 constraints값으로 분포를 만들고, 다른 하나는 이상적인 Gaussian 분포로 만들어 outlier를 제거하는 방식을 사용한다. 따라서 Data Association이 모호할 때 사용하면 굉장히 효과적이다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/153817525-97477f3c-012a-47b3-a14b-61ab3cde3138.png" width = "650" ></p>  

위와 같이 다양한 mode (빨간색, 파란색)에 대해서 서로 다른 분포를 만들고 Max mixture 방법을 사용한다.  

## 4. Dynamic Covariance Scaling  

두번째 방법은 Dynamic Covariance Scaling 방법이다.  

간단히 설명하면 이 방법은 constraints를 구성할 때 다양한 weight값들을 활용하여 outlier에 강인하도록 Error minimization을 해 나아가는 과정이다.  

보통의 Least squares는 아래와 같은 식을 활용한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/153819542-94a1388b-6aa0-4f30-86b1-5b7cccb136bd.png" width = "450" ></p>  

Dynamic Covariance Scaling 방법은 이 식에 Scale term ($s_{ij}$)을 추가하여 아래와 같이 식을 만드는 것이다. scale($s_{ij}$)도 다음과 같이 정의한다.   

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/153819801-c704d396-bb99-44a8-82e0-5a2b7dd24e0d.png" width = "500" ></p>  

각각의 observation마다 scale term을 추가하게 될 경우 outlier에 대한 값은 scale term을 통해 Error minimization을 하는데 적은 영향을 끼치게 할 수 있다.  

$ s_{ij} $를 정의하는데 사용한 $ \chi^2_{ij} $ 는 기존의 Least squares에서 사용하는 값을 나타내고, $ \Phi$ 는 사전에 정의한 parameter의 값이다. $ min $ 함수를 통해 1과 계산된 값에 더 작은 값을 $ s_{ij} $ 으로 정의한다.  

Error function을 시각화하면 아래와 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/153821236-5ce11f59-11a8-4318-96c3-b64df03e70bf.png" width = "500" ></p>  

검은색 line은 기존의 Error function, 파란색 line은 scale factor, 빨간색 line은 Dynamic Covariance Scaling 방법을 적용한 Error function이다.  

파란색 값이 1일 경우, 기존의 검은색 line과 빨간색 line은 같은 값을 가지게 되지만, 파란색 값이 작아질 경우(중심에서 멀어진 값들), Error에 대한 값도 작아져 outlier에 강인하도록 식이 설계됐다.  

**달라진 기울기는 선형화, Jacobian을 만들 때도 영향을 미치게 된다.**  

강의에서는 Dynamic Covariance Scaling 방법은 다음에 소개할 방법인 Robust least squares estimation에 특별한 방법으로 이해하면 된다고 했다.   

## 5. Least Squares with Robust Kernels  


---
title : "Slam 1강 (Introduction to SLAM) 요약"
category :
    - Slam
tag :
    - Slam
toc : true
toc_sticky: true
comments: true
---

Cyrill 교수님의 SLAM 강의를 듣고 정리해보자!  

## 0. Today's Goal
- SLAM이란 무엇인가?  
- Offline SLAM vs Online SLAM
- SLAM이 왜 어려운가?
- SLAM의 여러 Pipelines  

## 1. 강의 및 강의 자료  

강의 List는 [SLAM KR](https://www.youtube.com/channel/UCXvT7auo7xUd7v0B2pmvwIA)의 SLAM DUNK seson 2에서 정해준 강의를 따라 정리를 할 것이다.  

SLAM DUNK season 2의 목차는 [여기](https://youtube.com/playlist?list=PLubUquiqNQdMYwQVftUSFEWhJgzBErO9N)서 확인해 볼 수 있다.  

SLAM KR에서 이 강의를 듣고 요약을 세미나 형식으로 준비를 해준다!

이번 포스팅은 다음의 강의들을 참고하여 요약을 해보았다.  

- [Introduction to SLAM (Cyrill Stachniss, 2020)](https://www.youtube.com/watch?v=0I30M6yTklo)  
- [SLAM DUNK Season 2 - Introduction to SLAM](https://www.youtube.com/watch?v=d1TvU_jvMsE&list=PLubUquiqNQdP_H6uUmU-9f0y_LheA3Hil&index=1)  

Cyrill Stachniss의 강의 자료는 pdf로 [다운](https://drive.google.com/file/d/1aUgC8A7LwQleEBmAJMs9l6NyDKMVBwE0/view?usp=sharing) 받을 수 있다.  

이번 Lecture에서는 다음의 내용을 중점적으로 강의를 진행 될 예정이다.  
- **Graph-based SLAM using pose graphs** - 
    최근 SLAM 기술은 Graph-based 기반의 SLAM으로 많은 연구들이 이루어지고 있다.  
- **Graph-based SLAM with landmarks** - 
    Map을 만들 때 dense한 map보다는 Landmark들이 어디 있는지 기준으로 map을 만든다.  
    2D laser range scanner를 기반으로 어떻게 2D SLAM을 만드는지 배워보자.  
- **Robust optimization in SLAM** - 
    Outliers에 강인하도록 SLAM system을 만들자.  
- **State estimation using cameras** - 
    로봇의 움직이면서 자신의 자세(pose)를 추정하는데 Vision(Camera) 정보를 이용하자.

## 2. SLAM이란 무엇인가? 

SLAM이란 **Simultaneous Localization and Mapping**의 줄임말이다.  
즉, 로봇이 움직이면서 동시에 자신의 위치와 자신의 주변에 무엇이 있는지 파악하는 것을 말한다.  

SLAM을 하기 위해서는 **Localization**도 잘해야 하고 **Mapping**도 잘해야 한다.  

Localization이란 <u>로봇의 위치를 추정하는 것</u>을 말하고,  
Mapping이란 <u>로봇 주위에 무엇이 있는지 Map을 만드는 것</u>을 말한다. 
Map은 Sparse한 map을 만들 것인지(Landmark만 표시), Dense한 map을 만들 것인지에 따라 또 나뉜다.   

우리가 Map을 만들기 위해서는 우리 위치가 어디있는지 알아야 한다.  
우리가 우리 위치를 알기 위해서는 Map이 필요하다!  
Mapping과 Localization은 서로 의존적이다...  

그래서 SLAM을 **chicken-or-egg problem**이라고 한다. (닭이 먼저냐, 달걀이 먼저냐..)  

먼저 로봇이 실제로 Localization 하는 것을 예를 들어 보면,  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/100404090-e4b80600-30a3-11eb-973b-01359d1c2820.png" width = "400" ></p>

- 별표는 Landmark
- 흰색 동그라미는 모바일 로봇 - 이상적인 localization
- 회색 동그라미는 모바일 로봇 - 실제 localization  

실제로는 localization에는 다음의 이유로 error가 있다.  
1. 로봇이 측정하는 센서(ex. IMU)가 Noise가 있다.  
2. 로봇이 landmark를 관찰 했을 때, 측정 값(ex. laser, camera)가 Noise가 있다.  

그렇다면 Mapping을 할 때는 어떤지 예를 들어 살펴 보자.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/100404357-8b040b80-30a4-11eb-8ee3-b2f44affc0bf.png" width = "450" ></p>  

- 노란색 별표는 실제 Landmark  
- 회색 별포는 로봇이 측정한 Landmark  
- 흰색 동그라미는 모바일 로봇 - 이상적인 localization  

역시 Mapping을 할 때도 센서에 의존해서 Map을 만들기 때문에, error가 있을 수 밖에 없다.  

그럼 Mapping과 Localization을 동시에 해야 하는 SLAM은 어떨까?  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/100404650-3d3bd300-30a5-11eb-9eb9-d8d79f950e06.png" width = "400" ></p>  

더 잘 안된다... 

그럼 우리는 왜 SLAM system을 사용하려 하는걸까?  

- Localization은 정확한 landmark가 있어야 할 수 있다.  
- Mapping은 정확한 motion이 있어야 할 수 있다.  
- SLAM은 아무것도 없어도 바로 시작할 수 있다!  
- 이론적으로 'Loop closure'를 거치고 나면 error를 수정할 수 있다!  

## 3. Offline SLAM vs Online SLAM

SLAM system은 크게 2가지로 나뉘는데, 바로 Offline SLAM(Full SLAM)과 Online SLAM으로 나눈다.  

**Offline SLAM이란 우선, Sensor data를 모두 얻은 다음에 Map을 만드는 것**을 말한다.  
일반적으로 조금 더 정확한 Map을 만들고 싶을 때, Offline SLAM을 사용한다.  

**Online SLAM은 실시간으로 map도 만들고, 자기 자신의 위치도 알게 되는 SLAM system**을 말한다.  

수식적으로 보면 이 둘의 차이를 좀 더 이해하기 쉬운데, SLAM system을 수식적으로 표현해보자.  

우리가 측정하고 알 수 있는 것들은 다음과 같다.  
- Robot's contorl - 센서에서 odometry를 받아온 값 : $u_{1:T} = (u_1,u_2,u_3, ... , u_T)$
- Observations - laser나 camera 센서를 통해 관찰된 값 :   $z_{1:T} = (z_1,z_2,z_3, ... , z_T)$  

그리고 우리가 구해야하는 값들은 다음과 같다.  
- 환경의 Map 정보 - $m$  
- Robot의 Path 정보 - $x_{1:T} = (x_1,x_2,x_3, ... , x_T)$  

Localization과 Mapping 모두 Noise가 존재하기 때문에, 우리는 확률을 이용하여 표현한다.  

따라서 SLAM을 하나의 식으로 쓰면 다음과 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/100412117-0a023f80-30b7-11eb-840b-6bf803ce435e.png" width = "500" ></p>  

이 식을 해석(?)하면 **$z_{1:T}$와 $u_{1:T}$를 고려해서 $x_{1:T}$와 $m$을 확률적으로 표현**한다는 이야기이다.  

최근에는 Graph-based SLAM에 대한 연구가 많이 이루어지고 있는데, 이를 그림으로 표현하면 다음과 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/100412265-7c731f80-30b7-11eb-8d9e-bcd9d3998305.png" width = "500" ></p>  

지금까지 설명한 SLAM은 Offline SLAM이다.  
관찬할 수 있는 모든 센서의 값들을 가져와서 Map과 path를 계산하기 때문이다.  
Offline SLAM은 전체적인 경로를 만든다.

반면, Online SLAM은 현재에 관찰되는 $u_t$와 $z_t$만을 이용하여 Map과 Path를 만들어낸다.  
Online SLAM은 현재의 pose만 만들어 낸다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/100413567-0d97c580-30bb-11eb-8d86-96ace6989ff2.png" width = "500" ></p>  

Online SLAM에서 쓰이는 식을 모두 적분을 하게 되면 Offline SLAM처럼 만들 수 있다.  
즉, 현재 시점에서 구한 pose와 map을 계속 더하면 전체적인 경로 및 Map을 만들어낼 수 있다는 이야기다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/100413997-11781780-30bc-11eb-8235-c8bb117047c4.png" width = "500" ></p> 

## 4. SLAM이 왜 어려운가?  

SLAM이 어려운 이유는 **불확실성**때문이다.  
Map을 이용해서 Localization을 하고, localization을 이용해서 Map을 만드는데,  
둘 다 모르는 상태에서 구해야 한다..  

예를 들어 설명해보자.  
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/100414154-62880b80-30bc-11eb-8e6c-ec35cd6a8b8b.png" width = "500" ></p> 

로봇이 위치할 확률을 빨간색 원으로 표시해보자.  
만약 로봇에서 관찰된 landmark가 2개 있을 때, 로봇의 pose에 따라서 관찰되는 landmark가 달라진다.  

이러한 문제점 때문에 우리는 **Data association**이 필요하다.  

Data association이란 간단히 설명하자면 로봇이 어떠한 pose가 있을 때, 볼 수 있는 landmark들을 매칭시키는 작업을 말하는 것이다.  

## 5. SLAM의 여러 Pipelines 

Pipeline이 여러 의미로 쓰이지만, 여기서는 여러 종류의 기법들이라고 생각하면 쉬울 것 같다.  

SLAM을 푸는 방법을 크게 3가지로 나누어놨다.  

1. Kalman Filter
2. Particle Filter
3. Graph-based SLAM  

아까도 언급했지만 이 강의에서는 Grpah-based SLAM을 위주로 설명을 한다.  

Grpah-based SLAM은 Motion model과 Observation model을 이용해서 풀게 된다.  

다시 그림으로 표현하게 된다면 아래와 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/100415126-ba277680-30be-11eb-8b5a-d8131728614f.png" width = "500" ></p>

**Motion Model로 구하는 것은 로봇의 pose**이다.  
내 이전 pose와 encoder같은 것들로 관찰되는 control 값들을 통해 현재의 pose를 확률로 표현한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/100415257-18ecf000-30bf-11eb-8f74-1862e0f519cb.png" width = "400" ></p>

확률 분포를 Gaussian Model로 표현할 수도 있고, Non-Gaussian model로도 표현할 수 있다.  

**Observation Model로 구하는 것은 Map(landmark의 위치)**이다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/100415479-9284de00-30bf-11eb-8d59-2f84e85e1eb4.png" width = "350" ></p>  

마찬가지로 확률 분포를 Gaussian 아니면 Non-Gaussian model로 표현할 수 있다.  
















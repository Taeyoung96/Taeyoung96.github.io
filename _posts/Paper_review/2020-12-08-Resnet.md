---
title : "Resnet paper review"
category :
    - 논문정리
tag :
    - 논문
toc : true
comments: true
toc_sticky: true
---
'Resnet'을 읽고 정리해보자.  

딥러닝을 배우다 보면, 가장 기본적으로 배우는 구조 중 하나인 **ResNet**에 대해서 알아보자.  
Resnet의 논문 제목은 'Deep Residual Learning for Image Recognition'이다.  
Resnet은 **ILSVRC 2015 Classification task**에서 <u>우승을 한 모델</u>이다.  

## 0. 논문 

본 논문은 CVPR 2016에서 소개되었으며, 현재 인용 수가 6만 건이 넘는다. :open_mouth:  
원본 논문에 대한 자료는 [arXiv](https://arxiv.org/abs/1512.03385)에서 확인할 수 있다.  

Tensorflow, Pytorch 등 여러 프레임워크에서 resnet 구조를 제공해준다.  
구조를 직접 구현하여 강의 및 소스 코드를 올린 곳도 아주 많다.  

우선 논문의 내용을 요약해보자!  

## 1. Introduction

CNN에 관한 연구가 활발히 진행되던 중, network의 깊이가 결과에 큰 영향을 미친다는 사실을 알게 되었다. 따라오는 궁금증은 **그렇다면, 네트워크 깊이를 많이 쌓으면 쌓을 수록, 학습은 무조건 좋게 나오는 것일까?** 이다.  

하지만, 아래의 그림에서 나와있듯이, 네트워크 깊이가 깊을 수록 오히려 학습에 방해가 된다는 것을 알 수 있다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/101486116-84786c80-399f-11eb-90cb-2b9c9cf959ab.png" width = "500" ></p>

Training을 할 때도, Test를 할 때도 error가 더 높다.  

이러한 원인이 발생하는 이유는  
첫 번째로 **Vanishing / exploding gradients problem**이 있기 때문이다.  

하지만 이 문제는 여러 연구를 통해 어느정도 해결이 되었다.  
여러 가중치를 초기화시키는 기법들(Xavier/He initialization)과,  
Batch Normalization 등 다양한 기법들로 문제를 해결하였다.  

두 번째로는 **Degradation(퇴보)** 이다.  
이 문제는 Overfitting과는 살짝 다른데, 단지 네트워크의 깊이가 깊을 수록 최적화가 잘 되지 않는 현상이라고 생각하면 된다.  

보통 얕은 네트워크로 학습을 했을 때보다 깊은 네트워크로 학습을 했을 때, 학습이 더 잘 이루어 질 것이라고 생각하는데, Degradation 문제가 일어나 그렇지 않다는 것을 실험을 통해 보여주었다.  

이 두 번째 문제를 해결하기 위해 제안된 방법이 바로 **Resnet**에서 제안하는 방법이다.  

이 논문에서는 **Deep residual learning** 방법을 제안한다.  
이 방법을 **Skip connection** 또는 **Shortcut connection**이라고도 부른다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/101488135-8d1e7200-39a2-11eb-88d6-c41f077874de.png" width = "400" ></p>

$H(x) = F(x) + x$라고 할 때,  
이전의 경우에는 하나의 layer를 거치면 $H(x)$를 이용하여 학습을 시키는 경우였다.  
하지만 본 논문에서는 $H(x) - x$ 즉, $F(x)$만을 학습시켜 최적화가 더 잘 되게끔 하는 것을 제안한다.  

여기서는 $H(x)$보다 잔차 신호 $F(x)$를 최적화 시키는 것이 더 쉽다고 가정한다.  
그리고 논문에서 실제로 실험을 통해 이 가정을 증명한다.  

이 논문이 좋은 논문인 이유는 크게 2가지이다.  
1. 이 논문의 구조를 구현하는데 추가적인 parameter 및 계산 복잡도가 들지 않는다.  
2. 구현이 무척 쉽다.  

그렇다면 어떻게 이런 생각을 하게 되었을까? Relative Work를 통해 힌트를 얻어보자!  

## 2. Related Work 






---
title : "Resnet paper review"
category :
    - Paper_Review
tag :
    - Paper_Review
toc : true
comments: true
toc_sticky: true
---
'Resnet'을 읽고 정리해보자.  

딥러닝을 배우다 보면, 가장 기본적으로 배우는 구조 중 하나인 **ResNet**에 대해서 알아보자.  
Resnet의 논문 제목은 'Deep Residual Learning for Image Recognition'이다.  
Resnet은 **ILSVRC 2015 Classification task에서 우승을 한 모델**이다.  

## 0. 논문 

본 논문은 <font color='#1E90FF'> CVPR 2016 </font>에서 소개되었으며, 현재 인용 수가 6만 건이 넘는다. :open_mouth:  
원본 논문에 대한 자료는 [arXiv](https://arxiv.org/abs/1512.03385)에서 확인할 수 있다.  

Tensorflow, Pytorch 등 여러 프레임워크에서 resnet 구조를 제공해준다.  
구조를 직접 구현하여 강의 및 소스 코드를 올린 곳도 아주 많다.  

우선 논문의 내용을 요약해보자!  

## 1. Introduction

CNN에 관한 연구가 활발히 진행되던 중, network의 깊이가 결과에 큰 영향을 미친다는 사실을 알게 되었다. 따라오는 궁금증은 **그렇다면, 네트워크 깊이를 많이 쌓으면 쌓을 수록, 학습은 무조건 좋게 나오는 것일까?** 이다.  

하지만, 아래의 그림에서 나와있듯이, 네트워크 깊이가 깊을 수록 오히려 학습에 방해가 된다는 것을 알 수 있다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/101486116-84786c80-399f-11eb-90cb-2b9c9cf959ab.png" width = "500" ></p>

Training을 할 때도, Test를 할 때도 error가 더 높다.  

이러한 원인이 발생하는 이유는  
첫 번째로 **Vanishing / exploding gradients problem**이 있기 때문이다.  

하지만 이 문제는 여러 연구를 통해 어느정도 해결이 되었다.  
여러 가중치를 초기화시키는 기법들(Xavier/He initialization)과,  
Batch Normalization 등 다양한 기법들로 문제를 해결하였다.  

두 번째로는 **Degradation(퇴보)** 이다.  
이 문제는 Overfitting과는 살짝 다른데, 단지 네트워크의 깊이가 깊을 수록 최적화가 잘 되지 않는 현상이라고 생각하면 된다.  

보통 얕은 네트워크로 학습을 했을 때보다 깊은 네트워크로 학습을 했을 때, 학습이 더 잘 이루어 질 것이라고 생각하는데, Degradation 문제가 일어나 그렇지 않다는 것을 실험을 통해 보여주었다.  

이 두 번째 문제를 해결하기 위해 제안된 방법이 바로 **Resnet**에서 제안하는 방법이다.  

이 논문에서는 **Deep residual learning** 방법을 제안한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/101488135-8d1e7200-39a2-11eb-88d6-c41f077874de.png" width = "400" ></p>

$H(x) = F(x) + x$라고 할 때,  
이전의 경우에는 하나의 layer를 거치면 $H(x)$를 이용하여 학습을 시키는 경우였다.  
하지만 본 논문에서는 $H(x) - x$ 즉, $F(x)$만을 학습시켜 최적화가 더 잘 되게끔 하는 것을 제안한다.  

여기서는 $H(x)$보다 잔차 신호 $F(x)$를 최적화 시키는 것이 더 쉽다고 가정한다.  
그리고 논문에서 실제로 실험을 통해 이 가정을 증명한다.  

이 논문이 좋은 논문인 이유는 크게 2가지이다.  
1. 이 논문의 구조를 구현하는데 추가적인 parameter 및 계산 복잡도가 들지 않는다.  
2. 구현이 무척 쉽다.  

그렇다면 어떻게 이런 생각을 하게 되었을까? Relative Work를 통해 힌트를 얻어보자!  

## 2. Related Work 

- **Residual Representations**  

Image recognition 분야에서 벡터 양자화를 하는 과정이 존재한다. 이 과정에서 **Residual vectors를 이용하여 encoding을 하는 기법이 Original vectors를 이용하여 encoding을 하는 것보다 훨씬 더 효율적인 방법**이다.  

또한, PDE(편미분 방정식)를 푸는데 Multigrid method가 많이 사용되고 있는데, Multigrid method에서는 하나의 system을 subproblems로 나누어 문제를 재구성하고 subproblems들을 각각 해결하고 있다.  

이러한 방법들이 훨씬 더 빠르고 효율적이라는 것을 여러 논문에서 증명해오고 있다.  

- **Shortcut Connections**  

Shortcut connection에 관한 연구도 예전부터 진행되어 왔던 주제이다. MLP(Multi layer perceptrons)에서 vanishing/exploding gradients를 해결하기 위해서 shortcut connection 기법을 사용했었다. 최근 GoogleNet에서는 **Inception layer**를 이용하여 더 깊은 layer를 구성하였다.  

우리의 연구와 비슷하게 RNN 분야의 LSTM도 shortcut connection과 gate function을 사용하는데, 이는 Gate들은 Data에 의존적이고 파라미터가 추가적으로 필요하다.  

하지만 ResNet의 shortcut connection은 모든 정보가 어떤 방향(일직선)을 향하여 학습을 진행하기 때문에 훨씬 더 효과적이다.  

## 3. Deep Residual Learning

Introduction에서도 언급했지만 본 논문에서는 $H(x)$보다 잔차 신호 $F(x)$를 최적화 시키는 것이 더 쉽다고 가정한다. 그리고 $H(x)$를 학습시키기 보단, $H(x) - x = F(x)$를 학습을 진행한다.  

이런 식으로 식을 재구성하는 이유는 **Degradation**(퇴보) 때문이다.  
보통 layer가 더 깊게 쌓이면, 학습이 더 잘 진행되어야 할 것 같은데 실험으로 확인해 본 결과 그렇지 않기 때문이다.  











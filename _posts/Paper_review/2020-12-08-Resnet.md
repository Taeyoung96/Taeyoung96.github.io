---
title : "Resnet paper review"
category :
    - Paper_Review
tag :
    - Paper_Review
toc : true
comments: true
toc_sticky: true
---
'Resnet'을 읽고 정리해보자.  

딥러닝을 배우다 보면, 가장 기본적으로 배우는 구조 중 하나인 **ResNet**에 대해서 알아보자.  
Resnet의 논문 제목은 'Deep Residual Learning for Image Recognition'이다.  
Resnet은 **ILSVRC 2015 Classification task에서 우승을 한 모델**이다.  

## 0. 논문 

본 논문은 <font color='#1E90FF'> CVPR 2016 </font>에서 소개되었으며, 현재 인용 수가 6만 건이 넘는다. 😮    
원본 논문에 대한 자료는 [arXiv](https://arxiv.org/abs/1512.03385)에서 확인할 수 있다.  

Tensorflow, Pytorch 등 여러 프레임워크에서 resnet 구조를 제공해준다.  
구조를 직접 구현하여 강의 및 소스 코드를 올린 곳도 아주 많다.  

우선 논문의 내용을 요약해보자!  

## 1. Introduction

CNN에 관한 연구가 활발히 진행되던 중, network의 깊이가 결과에 큰 영향을 미친다는 사실을 알게 되었다. 따라오는 궁금증은 **그렇다면, 네트워크 깊이를 많이 쌓으면 쌓을 수록, 학습은 무조건 좋게 나오는 것일까?** 이다.  

하지만, 아래의 그림에서 나와있듯이, 네트워크 깊이가 깊을 수록 오히려 학습에 방해가 된다는 것을 알 수 있다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/101486116-84786c80-399f-11eb-90cb-2b9c9cf959ab.png" width = "500" ></p>

Training을 할 때도, Test를 할 때도 error가 더 높다.  

이러한 원인이 발생하는 이유는  
첫 번째로 **Vanishing / exploding gradients problem**이 있기 때문이다.  

하지만 이 문제는 여러 연구를 통해 어느정도 해결이 되었다.  
여러 가중치를 초기화시키는 기법들(Xavier/He initialization)과,  
Batch Normalization 등 다양한 기법들로 문제를 해결하였다.  

두 번째로는 **Degradation(퇴보)** 이다.  
이 문제는 Overfitting과는 살짝 다른데, 단지 네트워크의 깊이가 깊을 수록 최적화가 잘 되지 않는 현상이라고 생각하면 된다.  

보통 얕은 네트워크로 학습을 했을 때보다 깊은 네트워크로 학습을 했을 때, 학습이 더 잘 이루어 질 것이라고 생각하는데, Degradation 문제가 일어나 그렇지 않다는 것을 실험을 통해 보여주었다.  

이 두 번째 문제를 해결하기 위해 제안된 방법이 바로 **Resnet**에서 제안하는 방법이다.  

이 논문에서는 **Deep residual learning** 방법을 제안한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/101488135-8d1e7200-39a2-11eb-88d6-c41f077874de.png" width = "400" ></p>

$H(x) = F(x) + x$라고 할 때,  
이전의 경우에는 하나의 layer를 거치면 $H(x)$를 이용하여 학습을 시키는 경우였다.  
하지만 본 논문에서는 $H(x) - x$ 즉, $F(x)$만을 학습시켜 최적화가 더 잘 되게끔 하는 것을 제안한다.  

여기서는 $H(x)$보다 잔차 신호 $F(x)$를 최적화 시키는 것이 더 쉽다고 가정한다.  
그리고 논문에서 실제로 실험을 통해 이 가정을 증명한다.  

이 논문이 좋은 논문인 이유는 크게 2가지이다.  
1. 이 논문의 구조를 구현하는데 추가적인 parameter 및 계산 복잡도가 들지 않는다.  
2. 구현이 무척 쉽다.  

그렇다면 어떻게 이런 생각을 하게 되었을까? Relative Work를 통해 힌트를 얻어보자!  

## 2. Related Work 

- **Residual Representations**  

Image recognition 분야에서 벡터 양자화를 하는 과정이 존재한다. 이 과정에서 **Residual vectors를 이용하여 encoding을 하는 기법이 Original vectors를 이용하여 encoding을 하는 것보다 훨씬 더 효율적인 방법**이다.  

또한, PDE(편미분 방정식)를 푸는데 Multigrid method가 많이 사용되고 있는데, Multigrid method에서는 하나의 system을 subproblems로 나누어 문제를 재구성하고 subproblems들을 각각 해결하고 있다.  

이러한 방법들이 훨씬 더 빠르고 효율적이라는 것을 여러 논문에서 증명해오고 있다.  

- **Shortcut Connections**  

Shortcut connection에 관한 연구도 예전부터 진행되어 왔던 주제이다. MLP(Multi layer perceptrons)에서 vanishing/exploding gradients를 해결하기 위해서 shortcut connection 기법을 사용했었다. 최근 GoogleNet에서는 **Inception layer**를 이용하여 더 깊은 layer를 구성하였다.  

우리의 연구와 비슷하게 RNN 분야의 LSTM도 shortcut connection과 gate function을 사용하는데, 이는 Gate들은 Data에 의존적이고 파라미터가 추가적으로 필요하다.  

하지만 ResNet의 shortcut connection은 모든 정보가 어떤 방향(일직선)을 향하여 학습을 진행하기 때문에 훨씬 더 효과적이다.  

## 3. Deep Residual Learning

Introduction에서도 언급했지만 본 논문에서는 $H(x)$보다 잔차 신호 $F(x)$를 최적화 시키는 것이 더 쉽다고 가정한다. 그리고 $H(x)$를 학습시키기 보단, $H(x) - x = F(x)$를 학습을 진행한다.  

이런 식으로 식을 재구성하는 이유는 **Degradation**(퇴보) 때문이다.  
보통 layer가 더 깊게 쌓이면, 학습이 더 잘 진행되어야 할 것 같은데 실험으로 확인해 본 결과 그렇지 않기 때문이다.  

그 이유는 identitiy mapping을 할 때, Multiple nonlinear layer에 의해서 생기는 어려움이라고 연구자들은 생각한다. 따라서 재구성된 식을 이용하여 $F(x)$를 0으로 만들도록 학습하는 것이 이 문제를 해결하는데 도움을 준다는 것이다.  

---

하나의 Layer를 구성하는데 Block을 쌓아가는 형식으로 정의한다.  
**$x$와 $y$의 차원이 같을 때, $y = F(x,({W_i})) + x$로 정의**한다.  
$F(x,({W_i}))$는 residual mapping에 대해서 나타낸 것이다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/101488135-8d1e7200-39a2-11eb-88d6-c41f077874de.png" width = "400" ></p>

이 그림을 이용하여 설명을 한다면, 중간에 나와있는 $F(x)$는 $W_2\sigma(W_1 x)$로 나타낼 수 있고, $\sigma$는 ReLU와 같은 활성화 함수를 뜻한다.  

그리고 $F(x) + x$는 shortcut connection과 그 둘을 합한 것을 의미한다.  

이렇게 하나의 Block을 완성하고 이를 겹겹이 쌓아가는 것으로 ResNet을 구성할 수 있다.  

$x$와 $y$의 차원이 다를 때는 다음과 같이 정의한다.  
**$y = F(x,({W_i})) + W_s x$** - 차원을 맞춰주기 위해 $W_s$를 곱한 것 뿐이다.  

$F$는 유동적으로 만들 수 있지만, layer가 1개일 경우 reidual learning의 이점을 이용할 수 없다.  

제안한 식은 Fully connected layer뿐만 아니라, Multiple convolution layer, feature map, channel by channel 연산에도 적용을 할 수 있다.  

---

ResNet의 파워(?)를 입증하기 위해 Plain Network와 Residual Network 2개의 Network를 비교하여 실험을 진행하였다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/102006656-f79a2e00-3d65-11eb-920a-613e5bbc2d7c.png" width = "300" ></p>  

왼쪽은 VGG19의 network architecture이다. 기본적으로 3x3 filter를 사용하고 있으며 같은 크기의 feature map을 만들기 위해 동일한 filter의 갯수를 사용하였다. 또한 feature map의 크기가 절반으로 줄어들 경우 필터의 갯수를 2배로 늘렸는데 그 이유는 하나의 layer 당 시간 복잡도를 유지하기 위해서다.  

가운데에 있는 Plain Network는 Residual Network와 비교를 위해 만든 Baseline이다. VGG network의 영감을 받아 만들었으며, Downsampling을 할 때, stride를 2로 하고, Fully connected layer 대신 Global Average pooling을 이용하여 network를 설계하였다.  

그 결과 기존이 VGG Network보다 FLOP의 수가 줄어든 모습을 확인할 수 있는데, **FLOP이란 부동 소수점 연산을 의미하는데 파라미터의 수가 적을수록 FLOP이 적게 나타난다.**  

오른쪽에는 Residual Network의 network architecture이다. Plain Network에서 Shorcut connection을 추가한 형태이다. 중간중간 점선으로 되어있는 부분은 차원이 늘어날 때 shortcut connection을 해준 부분이다. 이 때는 $y = F(x,({W_i})) + W_s x$를 이용하여 차원을 맞춰주었다.  

---

실험은 ImageNet으로 진행을 하였다. VGG 논문에서 실험을 진행한 것 처럼 이미지를 224x224로 crop하고 horizontal flip, color augmentation을 진행하였다.

Batch Normalization을 적용하고, Loss function으로는 SGD를 사용하였다. Weigth decay는 0.0001, momentum은 0.9를 적용하였다. Learning rate는 0.1로 초기화를 하고, 정체가 나타나면 10으로 나눠줍니다.  

Test를 할때는 Alexnet을 평가할 때 수행했던 것처럼 10-crop test를 진행하고, Multiple scale에 대해서 평균적으로 점수를 매겼습니다.  

## 4. Experiments  

ImageNet을 위한 Architecture를 표로 정리를 했다. 위에 나온 그림은 34-layer일때 architecture를 나타낸 것이다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/102007439-74c8a180-3d6c-11eb-8a04-7570976acf92.png" width = "700" ></p> 

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/102007478-c07b4b00-3d6c-11eb-803b-c22ffd0fa0e9.png" width = "700" ></p> 

**Figure 4에서 확인할 수 있듯이, Plain Network 같은 경우, layer가 더 깊을 수록 Training을 하는데 error가 더 큰 반면, Residual Network 같은 경우, layer가 더 깊을 수록 Training 하는데 error가 더 줄어들었다.**  

본 논문의 연구자들은 이러한 이유에 대해서 더 깊은 plain network는 기하 급수적으로 수렴에 필요한 비율을 가질 수 있다고 추측합니다. 명확한 이유에 대해서는 밝혀지지 않은 것 같습니다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/102007599-e9501000-3d6d-11eb-87ca-7c1f77e04b38.png" width = "400" ></p> 

Table 2에서 보면 Validation set에서도 오른쪽 위 아래를 비교해보면 더 깊은 layer(34 layer)가 더 낮은 error를 보이고 있는데, 이를 통해 **Degradation problem이 ResNet을 통해 효과가 있다는 것을 보여주고 있습니다.**  

또한 왼쪽과 오른쪽의 결과를 비교해봐도, Plain Network보다 Residual Network가 error가 더 낮은 것을 확인할 수 있습니다.  

---

- **Identity vs Projection Shortcuts**  

Input과 Output layer의 차원이 다를 때 차원을 맞춰주기 위해서 projection shortcut을 사용하는데, 이것 또한 실험에 어떤 결과를 미치는지 한번 확인해보았다. 그 결과 **"Projection Shortcut은 Degradation problem을 해결하는데 꼭 필요한 필수적인 요소는 아니다."** 라는 결론을 지었다. 

- **Deeper Bottleneck Architectures**  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/102007828-8a8b9600-3d6f-11eb-870c-47e783e9eedf.png" width = "400" ></p> 










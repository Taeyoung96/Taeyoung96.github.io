---
title : "BTS paper review"
category :
    - Paper_Review
tag :
    - Paper_Review
toc : true
comments: true
toc_sticky: true
---

Depth estimation 논문인 BTS(From Big to Small)를 읽고 정리해보자!  

  
단일의 이미지로 어떻게 pixel 단위의 Depth 값을 추출할 수 있었는지 한 번 살펴보자!  

## 0. 논문 
최근 Monocular Depth estimation에 관심을 가지고 논문을 찾아보던 중  
성능이 좋은 편에 속하는 BTS(From Big to Small)이라는 논문을 접하게 되었다.  

이 논문은 <font color='#1E90FF'> CVPR 2019 </font>에서 소개되었으며 한국 사람이 쓴 논문이라 그런지 더 읽어보고 싶었다.

요약에 앞서 원본 논문에 대한 자료는 [arXiv](https://arxiv.org/abs/1907.10326)에서 확인할 수 있다.  
이 논문과 관련된 소스 코드도 [Github](https://github.com/cogaplex-bts/bts)에 공개되어 있다.  

Outdoor dataset으로 유명한 **KITTI Dataset**과  
Indoor dataset으로 유명한 **NYU Depth V2**를 이용하여 실험을 진행했는데,  
두 Dataset 모두 좋은 결과를 보이고 있다.  
(물론 모든 논문이 자신의 실험 결과가 가장 좋다고 이야기 하는 것 같다. 😏)  

## 1. Introduction

단일의 이미지로 정확한 Depth를 추정하는 일은  
**ill-posed problem**이 발생하기 때문에 굉장히 어려운 일이다.  

ill-posed-problem이란 번역하면 <u>불확정적 문제로 문제를 풀기 위한 조건이 너무 없을 때 쓰는 말</u>이다.  

Depth estimation의 경우도 전통적인 방법에선 여러 가지 조건들이 필요하다.  

예를 들어 기하학적인 정보도 필요하고 이미지의 경우 occlusions이 일어나면 안되며 이미지 Texture에 대한 이해도 필요하다.  

하지만 최근 딥러닝을 적용하여 꽤나 괜찮은 결과를 보인 논문들이 등장하고 있다.  

BTS도 **단일의 이미지를 이용하여 딥러닝을 통해 정확한 Depth값을 추출하는 법을 연구한 논문**이다.  
딥러닝 방법으론 **Supervised Learning** 기법을 사용하였다.  

이 논문에서는 하나의 딥러닝 모델을 제안하였는데 그 모델의 특징은 다음과 같다.  

1. **Encoder decoder 구조**  
2. Encoder에서는 **최근 Segmentation 기법들 중 Feature map을 잘 뽑는 방법**을 사용  
3. Decoder에서는 **Local Planar Guidance Layer**를 만들어 제안  

딥러닝을 이용한 Depth estimation을 하는 방법이 왜 효과가 있을까?  

일반적으로 <font color='ff0000'>인간이 Depth 값을 알아내기 위해 Local cue (물체에 대한 특징, 상대적인 Scale, 빛의 변화 등등) 와 Global Context (전체 Scene에 대한 이해)를 동시에 고려하여 Depth를 추정한다.</font>  

이러한 인간의 방법을 고려하여 'Deep Learning 모델을 설계한다면 Depth 값을 잘 측정하지 않을까?'  
라는 생각에서 딥러닝을 적용한 Depth estimation에 대한 연구가 꾸준히 진행되는 것 같다.  

## 2. Related Work  

### 2.1 Supervised Monocular Depth Estimation

Supervised Learning 기법을 이용하려면 Depth에 대한 Ground truth 값이 존재해야 한다.  
보통 RGB-D 카메라나 Multi-channel laser scanner를 이용하여 Ground truth 값을 얻어낸다.  

여러 연구들이 존재하지만 ICCV 2015에 발표된 [Eigen의 논문](https://openaccess.thecvf.com/content_iccv_2015/html/Eigen_Predicting_Depth_Surface_ICCV_2015_paper.html)에서 직접 Raw pixel value에서 Depth estimation을 진행하면서 크게 발전하게 되었다.  

또한 최근 연구에서 [DORN 모델](https://arxiv.org/abs/1806.02446)은 Regression problem 문제를 Quantized ordinal regression 문제로 해결하려는 방향으로 좋은 결과를 거두었다.  

### 2.2 Semi-Supervised Monocular Depth Estimation  

Supervised Learning 뿐만 아니라 다른 딥러닝 기법으로도 Depth estimation을 시도했었다.  

센서로 Ground truth 값을 얻어야 하는데 센서의 데이터 값이 오차가 날 수도 있고,  
수 많은 Ground truth 값을 얻는 것도 한계가 있기 때문이다.  

비슷한 이유로 **Self-Supervised learning 기법으로 Depth Estimation을 해결하려는 연구**도 진행중이다.  
직접적으로 Depth estimation을 문제를 해결하는 것이 아닌 Image Reconstruction 문제로 접근하였다.  

대표적으로 [Monodepth](https://arxiv.org/abs/1609.03677)와 [Monodepth2](https://arxiv.org/abs/1806.01260)라는 논문이 있다.  

하지만 이런 **Self-Supervised learning 방법은 Depth Map의 정확도 측면에서 Supervised Learning 기법보다 떨어지는 경향**이 있다.  

### 2.3 Video-Based Monocular Depth Estimation

Monocular Depth Estimation을 한 장의 이미지가 아닌 연속적인 이미지를 이용하여 접근하려는 연구도 있다.  

Pose Network와 Depth Network를 동시에 설계하고 End-to-end로 Pose와 Depth map을 동시에 구하는 방법이다.  

## 3. BTS Method  

먼저 Network Architecture부터 살펴보자.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105626649-3b5b7980-5e74-11eb-97b5-2806949951ff.png" width = "700" ></p>  

아까 언급했다시피 **이 모델은 Encoder - Decoder 구조**이다.  

Network Architecture에서 Encoder 부분은 다음과 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105626683-ac029600-5e74-11eb-80a2-b17db60141cf.png" width = "400" ></p>  

Encoder 방식을 통해 **resolution $H$의 크기를 $H/8$ 까지 줄인다.**  
여러 Classiciation 모델을 이용하여 Feature Map의 크기를 구하는데  
**MobileNetV2, ResNet, ResNext, DenseNet** 등을 이용한다.  

또한 <font color='ff0000'>ASPP(Atrous Spatial Pyramid Pooling) 기법을 이용</font>한다.  

ASPP란 Image Segmentation 분야에서 DeepLab v2 논문에서 처음 제안한 방법이다.  

Atrous convolution은 Receptive field를 넓혀주기 때문에 Segmenation 분야에서 Detail을 살려주기 위한 기법으로 많이 사용된다. Atrous convolution을 여러 Scale에 사용하여 Multi-scale에 잘 대응할 수 있는 Pooling 기법이 ASPP이다.  

ASPP에 대한 자세한 내용은 다음의 블로그를 참고해보자.  
- [ASPP: Atrous Spatial Pyramid Pooling - 꾸준희](https://eehoeskrap.tistory.com/459)  
- [ASPP(Atrous Spatial Pyramid Pooling)- JINSOL KIM](https://gaussian37.github.io/vision-segmentation-aspp/)  

Decoder 방식에서도 여러 기법들이 사용되는데,  
Upconvolution, Skip Connection 등 여러 논문에서 사용되던 방법들도 사용되고  

이 논문의 핵심 아이디어 중 하나인 <font color='ff0000'>Local Planar Guidance Layer</font>도 사용된다. 줄여서 LPG layer라고 부르겠다.  

LPG layer는 Depth estimation을 하는데 기하학적인 정보(Geometric guidance)를 제공하기 위해서 제안 된 layer이다. Upsampling을 단계적으로 진행하면서 여러 LPG layer를 사용하게 되고 resolution이 $H/8$인 feature map에서 최종적으로 resolution이 $H$인 결과를 얻을 수 있다.  

마지막으로 1x1 Convolution layer와 연산을 수행하면 최종적인 Depth estimation 값인 $\tilde{d}$ 값을 얻을 수 있다.  

### 3.1 Local Planar Guidance Layer  

Local Planar Guidance Layer의 모양은 다음과 같다.  


<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105628523-baef4580-5e80-11eb-9c15-b865f880b595.png" width = "400" ></p>

Input의 resolution이 $H/k$일 경우 $k*k$ 의 patch를 이용하여 resolution을 $H$로 키운다.  

LPG layer을 이용하여 Upsampling을 진행하게 되면 단 4개의 계수($n_1,n_2,n_3,n_4$)로 reconstruction이 가능해지는데, 일반적인 Upsampling 전략은 $k^2$ value가 필요한 것에 비해 매우 효과적인 방법이라 할 수 있다.  

먼저 resolution이 $H/k$인 Input이 들어오면 1x1 Convolution 연산을 통하여 channel의 수가 3이 되도록 맞춰준다.  

위 그림을 참고하면 두 가지 branch로 나누어 연산을 수행하는 것을 알 수 있다.  
첫번째 branch에서는 1st, 2nd channel을 이용하여 Equation 4에 따라 $n_1,n_2,n_3$을 구한다.  
(논문에서는 Equation 2라고 나와 있는데, 그림에선 Equation 4로 나와있다..  오타인 듯하다.)  

식은 다음과 같다.  
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105629186-e5430200-5e84-11eb-9953-f24ea7b7015b.png" width = "550" ></p>  

두번째 branch에서는 $n_4$를 구한다. Sigmoid function을 수행한 후,  
실제 real depth value를 구하기 위해 $\kappa$를 곱해준다.  

($n_1,n_2,n_3,n_4$)를 모두 구해준 후 다음과 같은 식을 이용하여 $\tilde{C_i}$를 구한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105629552-fd1b8580-5e86-11eb-95f2-dd50337298f2.png" width = "300" ></p>  

Input에 따라 다양한 Upsampling을 수행하여 얻어낸 $\tilde{C_i}$는 아래와 같다.  
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105629636-774c0a00-5e87-11eb-87c1-dca14a4a66c1.png" width = "400" ></p>

모든 $\tilde{C_i}$를 이용하여 최종적인 $\tilde{d}$를 구하는데 이용하는 식은 다음과 같다.  
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105629670-cb56ee80-5e87-11eb-9317-48026f616b8c.png" width = "500" ></p>  


### 3.2 Loss function  
[Eigen et al의 논문](https://arxiv.org/abs/1406.2283)에서 Scale-invariant error에 대한 Loss function을 제안하였는데,  
이 논문에서 살짝 변형하여 사용하고 있다.  
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105629771-5932d980-5e88-11eb-8ba7-a3bb562e0c62.png" width = "350" ></p>

여기서 $g_i$는 $log \tilde{d} - log d_i$를 나타내는데 $\tilde{d}$는 Depth Estimation 값이고 $d_i$는 Ground truth 값이다.  
$T$는 ground truth value 중 믿을만한 pixel의 갯수이다. (센서에서 취득한 값을 Ground truth로 사용하다보니 Outlier를 제거하기 위해 그런 것으로 추정한다.)  

위 논문에서는 error의 분산을 최소화하기 위해 $\lambda=0.85$를 사용한다.  

최종적인 Loss function $L$은 $\alpha \sqrt{\smash[b]{D(g)}}$를 사용하고 실험적으로 $\alpha = 10$을 사용한다.  

## 4. Result  

먼저 Outdoor dataset인 **KITTI dataset**을 이용하여 실험한 결과를 정리한 표는 다음과 같다.  
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105630708-0b20d480-5e8e-11eb-9a9b-18f4bc0ba360.png" width = "600" ></p>  

결과 사진은 다음과 같다.  
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105630845-a4e88180-5e8e-11eb-9539-dad2a3b295fe.png" width = "600" ></p> 

다음으로 Indoor dataset인 **NYU V2 depth dataset**에 대한 실험 결과는 다음과 같다.  
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105631016-9189e600-5e8f-11eb-80b6-786ded3cee13.png" width = "600" ></p> 

특이한 점은 여러 Backbone network를 바꿔가며 실험해 봤을 때, KITTI dataset 같은 경우 ResNext-101를 backbone으로 사용했을 때 가장 좋은 결과를 보였고 NYU V2 depth dataset 같은 경우 DenseNet-161을 이용했을 때 가장 좋은 결과를 보였다는 점이다.  

이렇게 BTS paper에 대한 Review를 마치도록 하겠다.  
아직 Monocular Depth Estimation은 Dataset에 대해 의존적인 경향이 있다.  
연구 발전 가능성이 많은 분야인 것 같다.



























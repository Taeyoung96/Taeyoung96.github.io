---
title : "BTS paper review"
category :
    - Paper_Review
tag :
    - Paper_Review
toc : true
comments: true
toc_sticky: true
---

Depth estimation 논문인 BTS(From Big to Small)를 읽고 정리해보자!  

  
단일의 이미지로 어떻게 pixel 단위의 Depth 값을 추출할 수 있었는지 한 번 살펴보자!  

## 0. 논문 
최근 Monocular Depth estimation에 관심을 가지고 논문을 찾아보던 중  
성능이 좋은 편에 속하는 BTS(From Big to Small)이라는 논문을 접하게 되었다.  

이 논문은 <font color='#1E90FF'> Arxiv preprint </font>에서 소개되었으며 한국 사람이 쓴 논문이라 그런지 더 읽어보고 싶었다.

요약에 앞서 원본 논문에 대한 자료는 [arXiv](https://arxiv.org/abs/1907.10326)에서 확인할 수 있다.  
이 논문과 관련된 소스 코드도 [Github](https://github.com/cogaplex-bts/bts)에 공개되어 있다.  

Outdoor dataset으로 유명한 **KITTI Dataset**과  
Indoor dataset으로 유명한 **NYU Depth V2**를 이용하여 실험을 진행했는데,  
두 Dataset 모두 좋은 결과를 보이고 있다.  
(물론 모든 논문이 자신의 실험 결과가 가장 좋다고 이야기 하는 것 같다. 😏)  

## 1. Introduction

단일의 이미지로 정확한 Depth를 추정하는 일은  
**ill-posed problem**이 발생하기 때문에 굉장히 어려운 일이다.  

ill-posed-problem이란 번역하면 <u>불확정적 문제로 문제를 풀기 위한 조건이 너무 없을 때 쓰는 말</u>이다.  

Depth estimation의 경우도 전통적인 방법에선 여러 가지 조건들이 필요하다.  

예를 들어 기하학적인 정보도 필요하고 이미지의 경우 occlusions이 일어나면 안되며 이미지 Texture에 대한 이해도 필요하다.  

하지만 최근 딥러닝을 적용하여  ill-posed problem을 해결하는 꽤나 괜찮은 결과를 보인 논문들이 등장하고 있다.  

BTS도 **단일의 이미지를 이용하여 딥러닝을 통해 정확한 Depth값을 추출하는 법을 연구한 논문**이다.  
딥러닝 방법으론 **Supervised Learning** 기법을 사용하였다.  

이 논문에서는 하나의 딥러닝 모델을 제안하였는데 그 모델의 특징은 다음과 같다.  

1. **Encoder decoder 구조**  
2. Encoder에서는 **최근 Segmentation 기법들 중 Feature map을 잘 뽑는 방법**을 사용  
3. Decoder에서는 **Local Planar Guidance Layer**를 만들어 제안  

딥러닝을 이용한 Depth estimation을 하는 방법이 왜 효과가 있을까?  

일반적으로 <font color='ff0000'>인간이 Depth 값을 알아내기 위해 Local cue (물체에 대한 특징, 상대적인 Scale, 빛의 변화 등등) 와 Global Context (전체 Scene에 대한 이해)를 동시에 고려하여 Depth를 추정한다.</font>  

이러한 인간의 방법을 고려하여 'Deep Learning 모델을 설계한다면 Depth 값을 잘 측정하지 않을까?'  
라는 생각에서 딥러닝을 적용한 Depth estimation에 대한 연구가 꾸준히 진행되는 것 같다.  

## 2. Related Work  

### 2.1 Supervised Monocular Depth Estimation

Supervised Learning 기법을 이용하려면 Depth에 대한 Ground truth 값이 존재해야 한다.  
보통 RGB-D 카메라나 Multi-channel laser scanner를 이용하여 Ground truth 값을 얻어낸다.  

여러 연구들이 존재하지만 ICCV 2015에 발표된 [Eigen의 논문](https://openaccess.thecvf.com/content_iccv_2015/html/Eigen_Predicting_Depth_Surface_ICCV_2015_paper.html)에서 직접 Raw pixel value에서 Depth estimation을 진행하면서 크게 발전하게 되었다.  

또한 최근 연구에서 [DORN 모델](https://arxiv.org/abs/1806.02446)은 Regression problem 문제를 Quantized ordinal regression 문제로 해결하려는 방향으로 좋은 결과를 거두었다.  

### 2.2 Semi-Supervised Monocular Depth Estimation  

Supervised Learning 뿐만 아니라 다른 딥러닝 기법으로도 Depth estimation을 시도했었다.  

센서로 Ground truth 값을 얻어야 하는데 센서의 데이터 값이 오차가 날 수도 있고,  
수 많은 Ground truth 값을 얻는 것도 한계가 있기 때문이다.  

비슷한 이유로 **Self-Supervised learning 기법으로 Depth Estimation을 해결하려는 연구**도 진행중이다.  
직접적으로 Depth estimation을 문제를 해결하는 것이 아닌 Image Reconstruction 문제로 접근하였다.  

대표적으로 [Monodepth](https://arxiv.org/abs/1609.03677)와 [Monodepth2](https://arxiv.org/abs/1806.01260)라는 논문이 있다.  

하지만 이런 **Self-Supervised learning 방법은 Depth Map의 정확도 측면에서 Supervised Learning 기법보다 떨어지는 경향**이 있다.  

### 2.3 Video-Based Monocular Depth Estimation

Monocular Depth Estimation을 한 장의 이미지가 아닌 연속적인 이미지를 이용하여 접근하려는 연구도 있다.  

Pose Network와 Depth Network를 동시에 설계하고 End-to-end로 Pose와 Depth map을 동시에 구하는 방법이다.  

## 3. BTS Method  

먼저 Network Architecture부터 살펴보자.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105626649-3b5b7980-5e74-11eb-97b5-2806949951ff.png" width = "700" ></p>  

아까 언급했다시피 **이 모델은 Encoder - Decoder 구조**이다.  

Network Architecture에서 Encoder 부분은 다음과 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105626683-ac029600-5e74-11eb-80a2-b17db60141cf.png" width = "400" ></p>  

Encoder 방식을 통해 **resolution $H$의 크기를 $H/8$ 까지 줄인다.**  
여러 Classiciation 모델을 이용하여 Feature Map의 크기를 구하는데  
**MobileNetV2, ResNet, ResNext, DenseNet** 등을 이용한다.  

또한 <font color='ff0000'>ASPP(Atrous Spatial Pyramid Pooling) 기법을 이용</font>한다.  

ASPP란 Image Segmentation 분야에서 DeepLab v2 논문에서 처음 제안한 방법이다.  

Atrous convolution은 Receptive field를 넓혀주기 때문에 Segmenation 분야에서 Detail을 살려주기 위한 기법으로 많이 사용된다. Atrous convolution을 여러 Scale에 사용하여 Multi-scale에 잘 대응할 수 있는 Pooling 기법이 ASPP이다.  

ASPP에 대한 자세한 내용은 다음의 블로그를 참고해보자.  
- [ASPP: Atrous Spatial Pyramid Pooling - 꾸준희](https://eehoeskrap.tistory.com/459)  
- [ASPP(Atrous Spatial Pyramid Pooling)- JINSOL KIM](https://gaussian37.github.io/vision-segmentation-aspp/)  

Decoder 방식에서도 여러 기법들이 사용되는데,  
Upconvolution, Skip Connection 등 여러 논문에서 사용되던 방법들도 사용되고  

이 논문의 핵심 아이디어 중 하나인 <font color='ff0000'>Local Planar Guidance Layer</font>도 사용된다. 줄여서 LPG layer라고 부르겠다.  

LPG layer는 Depth estimation을 하는데 기하학적인 정보(Geometric guidance)를 제공하기 위해서 제안 된 layer이다. Upsampling을 단계적으로 진행하면서 여러 LPG layer를 사용하게 되고 resolution이 $H/8$인 feature map에서 최종적으로 resolution이 $H$인 결과를 얻을 수 있다.  

마지막으로 1x1 Convolution layer와 연산을 수행하면 최종적인 Depth estimation 값인 $\tilde{d}$ 값을 얻을 수 있다.  

### 3.1 Local Planar Guidance Layer  

Local Planar Guidance Layer의 모양은 다음과 같다.  


<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105628523-baef4580-5e80-11eb-9c15-b865f880b595.png" width = "400" ></p>

Input의 resolution이 $H/k$일 경우 $k*k$ 의 patch를 이용하여 resolution을 $H$로 키운다.  

LPG layer을 이용하여 Upsampling을 진행하게 되면 단 4개의 계수($n_1,n_2,n_3,n_4$)로 reconstruction이 가능해지는데, 일반적인 Upsampling 전략은 $k^2$ value가 필요한 것에 비해 매우 효과적인 방법이라 할 수 있다.  

먼저 resolution이 $H/k$인 Input이 들어오면 1x1 Convolution 연산을 통하여 channel의 수가 3이 되도록 맞춰준다.  

위 그림을 참고하면 두 가지 branch로 나누어 연산을 수행하는 것을 알 수 있다.  
첫번째 branch에서는 1st, 2nd channel을 이용하여 Equation 4에 따라 $n_1,n_2,n_3$을 구한다.  
(논문에서는 Equation 2라고 나와 있는데, 그림에선 Equation 4로 나와있다..  오타인 듯하다.)  

식은 다음과 같다.  
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105629186-e5430200-5e84-11eb-9953-f24ea7b7015b.png" width = "550" ></p>  

두번째 branch에서는 $n_4$를 구한다. Sigmoid function을 수행한 후,  
실제 real depth value를 구하기 위해 $\kappa$를 곱해준다.  

($n_1,n_2,n_3,n_4$)를 모두 구해준 후 다음과 같은 식을 이용하여 $\tilde{C_i}$를 구한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105629552-fd1b8580-5e86-11eb-95f2-dd50337298f2.png" width = "300" ></p>  

Input에 따라 다양한 Upsampling을 수행하여 얻어낸 $\tilde{C_i}$는 아래와 같다.  
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105629636-774c0a00-5e87-11eb-87c1-dca14a4a66c1.png" width = "400" ></p>

모든 $\tilde{C_i}$를 이용하여 최종적인 $\tilde{d}$를 구하는데 이용하는 식은 다음과 같다.  
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105629670-cb56ee80-5e87-11eb-9317-48026f616b8c.png" width = "500" ></p>  


### 3.2 Loss function  
[Eigen et al의 논문](https://arxiv.org/abs/1406.2283)에서 Scale-invariant error에 대한 Loss function을 제안하였는데,  
이 논문에서 살짝 변형하여 사용하고 있다.  
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105629771-5932d980-5e88-11eb-8ba7-a3bb562e0c62.png" width = "350" ></p>

여기서 $g_i$는 $log \tilde{d} - log d_i$를 나타내는데 $\tilde{d}$는 Depth Estimation 값이고 $d_i$는 Ground truth 값이다.  
$T$는 ground truth value 중 믿을만한 pixel의 갯수이다. (센서에서 취득한 값을 Ground truth로 사용하다보니 Outlier를 제거하기 위해 그런 것으로 추정한다.)  

위 논문에서는 error의 분산을 최소화하기 위해 $\lambda=0.85$를 사용한다.  

최종적인 Loss function $L$은 $\alpha \sqrt{\smash[b]{D(g)}}$를 사용하고 실험적으로 $\alpha = 10$을 사용한다.  

## 4. Result  

먼저 Outdoor dataset인 **KITTI dataset**을 이용하여 실험한 결과를 정리한 표는 다음과 같다.  
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105630708-0b20d480-5e8e-11eb-9a9b-18f4bc0ba360.png" width = "600" ></p>  

결과 사진은 다음과 같다.  
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105630845-a4e88180-5e8e-11eb-9539-dad2a3b295fe.png" width = "600" ></p> 

다음으로 Indoor dataset인 **NYU V2 depth dataset**에 대한 실험 결과는 다음과 같다.  
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/105631016-9189e600-5e8f-11eb-80b6-786ded3cee13.png" width = "600" ></p> 

특이한 점은 여러 Backbone network를 바꿔가며 실험해 봤을 때, KITTI dataset 같은 경우 ResNext-101를 backbone으로 사용했을 때 가장 좋은 결과를 보였고 NYU V2 depth dataset 같은 경우 DenseNet-161을 이용했을 때 가장 좋은 결과를 보였다는 점이다.  

이렇게 BTS paper에 대한 Review를 마치도록 하겠다.  
아직 Monocular Depth Estimation은 Dataset에 대해 의존적인 경향이 있다.  
연구 발전 가능성이 많은 분야인 것 같다.



























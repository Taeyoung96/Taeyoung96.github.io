---
title : "YOLACT paper review"
category :
    - Paper_Review
tag :
    - Paper_Review
toc : true
comments: true
toc_sticky: true
---

'Yolact'를 읽고 간단히 요약해보았습니다.

**'YOLACT'** 란 **'You Only Look At CoefficienTs'** 의 줄임말로  
Real-time으로 Instatnce Segmentation이 동작하는 알고리즘에 대한 논문이다.  

## 0. 논문 
ICCV 2019에 발표되었으며   원본 논문에 대한 자료는 [arXiv](https://arxiv.org/abs/1904.02689)에서 확인할 수 있다.  
소스코드도 [dbolya/yolact](https://github.com/dbolya/yolact)에 공개되어 있다.  

## 1. Introduction

이 논문의 Introduction은 다음과 같은 말로 시작한다.
> “Boxes are stupid anyway though, I’m probably a true
believer in masks except I can’t get YOLO to learn them.”
– Joseph Redmon, YOLOv3

YOLO는 Object Detection 분야에서 real time으로 동작할 수 있는 알고리즘이다. 
이 YOLO Algorithm에서 영감을 얻어 논문을 작성한 것으로 추측이 된다.

Instance Segmentation이란 **각각 Pixel 별로 Labeling을 하는데 같은 Class이더라도 다른 Instance이면 서로 다른 Label을 가지도록 Segmentation을 하는 기법**을 말한다.

Instance Segmentation의 관련 연구로는 대표적으로 **Mask R-CNN**이 있다.  

하지만 Mask R-CNN 같은 경우, 기본적으로 ROI(관심 영역)을 찾은 다음 Class, Bounding Box, Mask 등을 예측하기 때문에 시간이 오래 걸린다는 단점이 있다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/96360873-f67cc400-115b-11eb-9067-77c332f6457a.png" width = "450" ></p>

위의 표를 보면 COCO dataset을 이용하여 Speed와 performance를 나타낸 표이다.  

FPS(x축)는 Frame per Speed로써 Speed를 나타내고,  
mAP(y축)는 Average Precision으로써 얼마나 정확한지를 나타내는 지표이다.  

처음에 AP에 대한 개념을 잡기 힘들었는데,  
[물체 검출 알고리즘 성능 평가방법 AP(Average Precision)의 이해](https://bskyvision.com/465)에  
설명을 잘 해놓았으니 참고해 보면 좋을 것 같다.  

Ours(YOLACT)가 mAP(정확도) 측면에서는 살짝 떨어지지만,  
FPS(Speed)가 다른 알고리즘들보다 훨씬 높게 측정되는 것을 확인할 수 있다.  

YOLACT는 기존의 방법인 2 stage instance segmentation 기법  
(ROI를 찾고 Class, Bounding Box, Mask를 찾는 방식)에서 벗어나,  
새로운 방법을 하나 제안하였다.  

> 1. Generating a dictionary of non-local prototype masks
> 2. Predicting a set of linear combination coefficients per instance

위 두가지 과정을 병렬적으로 처리를 하기 때문에 YOLACT는 실시간으로 동작하는 Instance Segmentation을 개발할 수 있었다.  

그렇다면 위 두가지 Main Idea가 어떤 것을 뜻하는지 한 번 알아보자.  

### 1. Generating a dictionary of non-local prototype Masks

여기서 Prototype이 어떤 것을 뜻하는지 알아야 한다.  

기본적으로 Prototype은 Computer Vision분야에서 특징들을 찾는 방식 중 하나인데,  
이러한 Prototype을 Mask를 찾는데 이용한다.  

아래의 그림을 보면 Prototype이 어떤건지 이해하기 쉬울 것이다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/96361319-aa338300-115f-11eb-8d69-52b65b28e2b3.png" width = "400" ></p>

가로축에 나와있는 a ~ f는 모두 다른 이미지를 뜻하고,  
세로축에 나와있는 1 ~ 6는 서로 다른 Prototype을 뜻한다.  

표에 노란색으로 나와있는 부분은 그 부분을 집중해서 볼 것이라는  뜻이고,  
파란색 부분은 집중해서 볼 것이 아니라는 뜻이다.
(나는 이렇게 이해했다..ㅎ)  

각각 다른 Prototype에 따라 같은 이미지라도 활성화 되는 부분이  
다른 것을 알 수 있다.

예를 들어 1번 Prototype 같은 경우 이미지의 왼쪽 부분을 집중해서 보는 것처럼  
나타나는 것을 알 수 있다.

그렇다면 **Generating a dictionary of non-local prototype masks**는  
어떤 의미인가? 

말 그대로 **서로 다른 Prototype Mask의 모음을 만든다**는 뜻이다.  

또 다시 논문에 나와있는 그림을 이용하여 설명해보면,  
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/96361908-8de61500-1164-11eb-81be-e86dd65cc5f6.png" width = "300" ></p>  

위 그림을 보면 서로 다른 Prototype으로 이루어져 있는 집합체(Dictionary)가  
나와 있는 것을 알 수 있다.  

이 Dictionary를 만들고 이용한다는 것이 **YOLACT의 첫 번째 Main Idea**이다.  

### 2. Predicting a set of linear combination coefficients per instance  

첫번째 Main Idea에서 만든 Dictionary를 어떻게 이용할 것인가?  
논문에 보면 Dictionary를 Set이라는 말과 비슷한 느낌으로 사용한 듯 하다.  

우선 Instance별로 Mask의 Coefficients(계수)를 예측한다.  

Mask의 Coefficients를 Instance별로 예측을 한다면,  
각각 Instance마다 최적의 Mask Coefficients를 사용할 수 있을 것이다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/96361908-8de61500-1164-11eb-81be-e86dd65cc5f6.png" width = "300" ></p>  

위 그림에 나와있는 Prototype Mask를 각각 하나의 벡터라고 생각하자.  
그렇다면 각각 $v_1, v_2, v_3, v_4$라고 생각할 수 있다.  
이러한 벡터들의 조합으로 우리는 두 가지의 결과를 얻으려고 한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/96362249-efa77e80-1166-11eb-9243-d5a036fad849.png" width = "200" ></p> 

첫번째 그림은 **사람**의 형태가 활성화 된 Prototype Mask이고,  
두번째 그림은 **라켓**의 형태가 활성화 된 Prototype Mask이다.  

위의 벡터들을 선형 결합하면 두 결과를 얻을 수 있을 것이다.  

$result Mask = w_1v_1 + w_2v_2 + w_3v_3 + w_4v_4$  

그렇다면 두 $resultMask$를 얻어내는 데 사용되는  
$w_1,w_2,w_3,w_4$ 값이 같을 것인가?  

조금만 생각해봐도 둘은 다른 값을 가질 것이라고 예상해볼 수 있다.  

이것이 **YOLACT의 두 번째 Main Idea**이다.  

각각 Instance마다 최적의 Mask를 찾기위해 서로 다른 Mask Coefficients  
($w_1,w_2,w_3,w_4$)를 예측한다는 것이다.  

이렇게 YOLACT의 두 가지 Main Idea에 대해서 이해를 해봤다.  

이제 YOLACT의 Architecture를 통해서 어떻게 YOLACT가 돌아가는지 살펴보자!  
## 2. Yolact Architecture

논문에 나와있는 'YOLACT Architecture'을 살펴보면 다음과 같다.  
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/96362460-685b0a80-1168-11eb-9684-9cd7eaa9566f.png" width = "700" ></p> 

총 4단계로 나누어 YOLACT의 구성을 한 번 살펴볼 것이다.  

> - Feature Backbone + Feature Pyramid
> - Protonet
> - Prediction Head
> - NMS, Crop, Threshold

### Feature Backbone + Feature Pyramid

위 그림에서는 Backbone Network를 ResNet-101을 이용한다.  

**Backbone Network**란 뼈대가 되는 Network이다.  
즉, 기존에 성능이 좋은 모델을 이용한다고 생각하면 된다.  

여기서는 ResNet-101의 총 5개의 Feature Map을 이용한다.  
각각 $C_1$~$C_5$로 정의하자.  

이러한 Feature Map을 이용하여 Feature Pyramid Network를 구성한다.  

Feature Pyramid Network에 대한 설명은  
[논문리뷰&재구현) Mask R-CNN 설명 및 정리](https://ganghee-lee.tistory.com/40)에서 Feature Pyramid Network Part에 자세히 나와있다.  

Feature Pyramid Network를 통해 $P_3$ ~ $P_7$까지 구성을 하였다.  

이 Feature Map들이 이제 다음 Network의 Input으로 들어가는데,  

Prediction Head Network의 경우 $P_3$ ~ $P_7$ 모두 Input으로 사용하고  
ProtoNet의 경우 $P_3$를 Input으로 이용한다.  

### Protonet

Protonet은 YOLACT의 Main Idea에서 **A dictionary of non-local prototype masks** 부분을 Network로 구현한 것이다.  

Input으로 Feature Pyramid Network의 $P_3$이 들어가면,  
Output으로 k개의 Prototype Mask가 만들어지게 된다.  

여기서 k는 Hyperparameter로 초기에 정해진 숫자만큼   
Prototype Mask가 만들어지게 된다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/96363474-9bed6300-116f-11eb-8af5-12409efa3e3e.png" width = "200" ></p>  

위 표를 보면 k값에 따라 결과가 달라지는 것을 볼 수 있다.  

Protonet의 Network Architecture를 보면 다음과 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/96363508-d5be6980-116f-11eb-90fd-a70c3b239a13.png" width = "400" ></p>  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/96363559-2930b780-1170-11eb-8938-f2dbc8734be4.png" width = "500" ></p> 

최종적으로 (Prototype Mask의 갯수, Width, Height)가  
(k,138,138)로 나오는 것을 알 수 있다.  

여기서 Loss는 따로 계산하지 않는데, 최종적인 Mask의 값이 정해졌을 때  
그 값을 이용하여 Weight들을 Update하기 때문이다.  

### Prediction Head

Prediction Head Network는 YOLACT의 Main Idea에서  
**Mask의 Coefficient의 계수를 예측**하는데 사용하는 Network이다.  

Architecture는 다음과 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/96363709-cab80900-1170-11eb-8c9c-904828f5203e.png" width = "400" ></p>

기존의 Object Detection에서 Anchor Box 기반으로 Class와 Bounding Box를 예측하는 Network와 유사한 구조로, 여기에 Mask Coefficient를 예측하는 값까지 추가가 되었다.  

어떤 클래스인지 예측하는 c,  
Bounding Box값을 나타내는데 사용할 4,  
Mask Coefficient의 값을 예측하는데 k개를 예측하여  

하나의 Anchor 당 `c+4+k`개를 예측하게 된다.  

이렇게 두 Network를 이용하여 찾은 Prototype Mask의 Set과  
Mask Coefficients들을 이용하여 최종적인 Mask를 구하게 된다.  

최종적인 Mask는 단순한 덧셈과 뺄셈을 이용하는데  
그 이유는 속도를 빠르게 하기 위함이라고 논문에 언급되어 있다.  

최종적인 Mask를 얻기위해 사용하는 식은 다음과 같다.  

$M = \sigma(PC^T)$

P는 prototype Mask를 나타내는 행렬이고 (h,w,k)  
C는 Mask Coefficient를 나타내는 행렬이고 (n,k)로 이루어져 있다.  

h는 높이, w는 너비, k는 prototype mask의 갯수,  
n은 NMS(non maximum suppression)의 Threshold로 사용된다.  

여기서 $\sigma$는 Sigmoid 함수를 의미하는데 최종 Mask의 비선형성을 부여하기 위해  
추가하였다.  

### NMS, Crop, Threshold  

1 . NMS

첫번째로 **NMS란 Non Maximum Suppression**을 뜻한다.  

Object Detection 분야에서 동일 object에 대해 여러 Bounding Box 중  
가장 score가 높은 Box만 남겨두기 위해 사용하는 Algorithm이다.      

YOLACT 논문에서는 기존에 사용하는 NMS대신 Fast NMS를 따로 제안했는데,  
이유는 기존 NMS 같은 경우 연산량이 많기 때문이다. 

실시간으로 동작해야 하는 YOLACT 특성상 기존에 존재한 Algorithm을 개선하여  
빠르게 동작할 수 있도록 만들었다.

**Fast NMS의 핵심은 IoU Matrix를 만드는 것이다.**  

여기서 뜻하는 IoU(Intersection of Union)은 Anchor Box 간의 IoU를 뜻하는데  
NMS와 Fast NMS의 차이점은 [(논문리뷰&재구현) YOLACT 설명 및 정리 - (3)](https://ganghee-lee.tistory.com/46?category=863370)에  
자세히 설명이 되어있어 포스팅을 참고하면서 이해하면 된다.  

2 . Crop

두번째로 **Crop**은 Instance마다의 Bounding Box를 나타내기 위해 거치는 단계이다.  

Training 과정에서는 기존의 Groundtrutu Bounding Box를 이용하여 Cropping을  
진행하고, Test 과정에서는 예측한 Bounding Box를 이용하여 Cropping을 진행한다.  

3 . Threshold

세번째로 Threshold는 말 그대로 경계값을 정해주는 단계이다.  
Threshold보다 크게 confidence가 예측이 되는 Bounding Box 및 Mask들만  
이미지에 출력을 할 수 있게 된다.  


## 3. Results

YOLACT를 사용하므로써 결과적으로  
Real time Instance Segmentation을 구현할 수 있다.  

아래의 그림은 논문에 나온 그림이다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/96366310-e546ae00-1181-11eb-9258-86a6c2d9060e.png" width = "700" ></p>

그림이 좀 작지만 자세히 보면 YOLACT를 이용하여 만든 Mask가  
훨씬 더 정밀하다는 것을 알 수 있다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/96366338-2474ff00-1182-11eb-93c0-c8464deb6592.png" width = "700" ></p>

위 표는 다른 Instance Segmentation Algorithm과 비교를 나타낸 표이다.  

Mask R-CNN보다 AP(Average Precision)은 좀 떨어지지만, FPS가 확실히 높기때문에 YOLACT도 충분히 활용을 할 수 있는 Algorithm이라 생각한다.  

## 4. References

- [YOLACT: Real-time Instance Segmentation](https://arxiv.org/abs/1904.02689)

- [논문 리뷰 - YOLACT: Real-time Instance Segmentation](https://byeongjokim.github.io/posts/YOLACT,-Real-time-Instance-Segmentation/)

- [(논문리뷰&재구현) YOLACT 설명 및 정리 - (1)](https://ganghee-lee.tistory.com/42?category=863370)

- [[CV2019/PaperSummary] YOLACT :Real-time Instance Segmentation](https://medium.com/@abhigoku10/cv2019-papersummary-yolact-real-time-instance-segmentation-e62fa721957f)


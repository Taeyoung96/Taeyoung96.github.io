---
title : "CS231n 6강 요약"
category :
    - CS231n
tag :
    - machine_learning
    - computer vision
    - CS231n
toc : true
comments: true
---

Neural Network를 Training 시킬 때, 고려해야 하는 것들을 알아보자!

## 0. Today's Goal
> - Neural Network Traing이란?
> - Activation Function
> - Data processing
> - Weight Initialization
> - Batch Normalization
> - 학습 과정을 설계하는 법
> - Hyperparameter Optimization

## 1. 강의 및 강의 자료

강의를 들어볼 수 있는 링크와 강의 자료를 pdf형식으로 준비했다.

- 강의 링크 : [video](https://www.youtube.com/watch?v=wEoyxE0GP2M&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=6)

- 강의 슬라이드 : [슬라이드](https://github.com/Taeyoung96/Taeyoung96.github.io/files/4975049/cs231n_2017_lecture6.pdf)

한글 자막이 필요하신 분은 다음의 링크를 확인해주세요. :)

- 한글 자막을 첨부하고 싶으면 [여기](https://github.com/visionNoob/CS231N_17_KOR_SUB)를 눌러주세요.

## 2. Neural Network Traing이란?

우리가 network parameter를 최적화하는 방법 중 Gradient Descent Algorithm에 대해서 배웠다.

그리고 모든 data를 가지고 gradient descent Algorithm에 적용을 하면 계산량이 많기 때문에 SGD(Stochastic Gradient Descent) Algorithm을 이용한다.
Sample을 뽑아내 Gradient Desscent Algorithm을 사용하는 방법이다.

처음에 모델을 어떻게 선정해야 하고, Training 할 때 유의해야할 사항은 무엇인지 평가는 어떻게 해야하는지 한 번 알아보자.

## 3. Activation Function

Activation Function의 종류에 대해서 알아보자.

첫번째로 나온 함수는 Sigmoid Function이다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88452221-e5dbeb00-ce97-11ea-8991-bf79b5e400d4.png" width = "400" ></p>

이 함수는 출력이 (0,1) 사이의 값이 나오도록 하는 선형 함수이다.

하지만 단점이 존재하는데,
1. Saturated neurons가 Gradient값을 0으로 만든다.
2. 원점 중심이 아니다.
3. 지수함수가 계산량이 많다.

**Saturate**를 찾아보면 '포화'라고 해석을 하는데, 입력이 너무 작거나 클 경우 값이 변하지 않고 일정하게 1로 수렴하거나 0으로 수렴하는 것을 포화라고 생각하고 Gradient의 값이 0인 부분을 의미하는 것 같다.

그렇다면 Gradient의 값이 0이 되는 것은 왜 문제가 될까?

> Chain Rule 과정을 생각했을 때, Global gradient값이 0이 되면 즉 결과 값이 0이 되면 local gradient 값도 0이 된다. 따라서 Input에 있는 gradient 값을 구할 수 없다.

그럼 원점 중심이 아닌 것은 왜 문제가 될까?
> output의 값이 항상 양수면 다음 input으로 들어갔을 때도 항상 양수이게 된다.
그렇다면 다음 layer에서 $w$의 값을 update할 때 항상 같은 방향으로 update가 된다.
다음 그림의 예로 설명하면 우리가 원하는 vector가 파란색일 때, 
$w$의 같은 경우 제 1사분면과 제 3사분면으로 update가 되기 때문에 우리가 원하는 방향으로 update를 하기 힘들다.
> <p align="center"><img src="https://user-images.githubusercontent.com/41863759/88456683-8e9b4200-ceba-11ea-9cfd-01287b0cf132.png" width = "400" ></p>

Sigmoid의 2번 단점을 보완하기 위해 나온 function이 바로 $tanh(x)$이다.
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88456715-f487c980-ceba-11ea-8243-c2ea1d53a610.png" width = "200" ></p>

하지만 여전히 saturated한 뉴런일 때, gradient값이 0으로 된다.

따라서 새로운 activation function이 제안되었는데, 바로 **ReLU**이다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88456825-0cac1880-cebc-11ea-83b6-3e293f4cb5ea.png" width = "300" ></p>

ReLU의 특징은 (+) 영역에서 saturate하지 않고, 계산 속도도 element-wise 연산이기 때문에 sigmoid/tanh보다 훨씬 빠르다는 특징이 있다.

하지만 ReLU에도 단점은 존재하는데, 바로 (-)의 값은 0으로 만들어 버리기 때문에 Data의 절반만 activate하게 만든다는 것이다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88456895-c0150d00-cebc-11ea-894d-55484d3681ff.png" width = "500" ></p>

그래서 이러한 단점을 보완하기 위해 여러 activation function이 또 제시 되었다.

- Leaky ReLU
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88456934-3e71af00-cebd-11ea-9f84-6d607ff6ac79.png" width = "300" ></p>

- Exponential Linear Unit
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88456991-d8395c00-cebd-11ea-8748-127af7ab1f3f.png " width = "300" ></p>

Maxout이라는 activation function도 있는데, 이 함수를 사용하려면 parameter가 기존 function보다 2배 있어야 한다.

$$ max(w^T_1x + b_1, w^T_2x + b_2)$$

## 4. Data processing

데이터 전처리 과정에서는 주로 Zero-centered, Normalized, PCA,
Whitening같은 처리들을 한다.

Zero-centered 나 Normalized를 하는 이유는 모든 차원이 동일한 범위에 있어 전부 동등한 기여를 할 수 있도록 하는 것이다.

PCA나 Whitening은 더 낮은 차원으로 projection하는 느낌인데 이미지 처리에서는 이런 전처리 과정은 거치지 않는다.

기본적으로 이미지는 **Zero-centered** 과정만 거친다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457141-11be9700-cebf-11ea-8ed1-f35c5425bdc7.png" width = "500" ></p>

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457162-34e94680-cebf-11ea-853a-9f8582c74dd5.png" width = "500" ></p>

실제 모델에서는 train data에서 계산한 평균을 test data에도 동일하게 적용한다.

## 5. Weight Initialization

초기값을 몇으로 잡아야 최적의 모델을 구할 수 있을까?

만약 초기값을 0으로 한다면, 모든 뉴런은 동일한 일을 하게 될 것이다.
즉 모든 gradient의 값이 같게 될 것이다. 이렇게 하는 것은 의미가 없다.

그래서 생각한 첫번째 Idea는 '작은 random한 수로 초기화'를 하는 것이다.

초기 Weight는 표준정규분포에서 Sampling을 한다.

하지만 이런 경우 얕은 network에서는 잘 작동을 하지만 network가 깊어질 경우 문제가 생긴다.

왜냐하면 network가 깊으면 깊을수록, weight의 값이 너무 작아 0으로 수렴하기 때문이다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457360-a5dd2e00-cec0-11ea-8896-14a76d80c4f5.png" width = "600" ></p>

만약 표준 편차를 키우게 되면 어떨까?

activaton value의 값이 극단적인 값을 가지게 되고, gradient의 값이 모두 0으로 수렴할 것이다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457422-2439d000-cec1-11ea-8f98-1270679171a6.png" width = "600" ></p>

이런 초기값 문제에 대해서 'Xavier initialization'이라는 논문이 제시 되었는데
일단 activation function이 linear하다는 가정하에 다음과 같은 식을 사용하여 weight의 값을 초기화 한다.
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457443-64994e00-cec1-11ea-8861-e20e8cc8a8af.png" width = "600" ></p>

이 식을 이용하면 입/출력의 분산을 맞춰줄 수 있게 된다.
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457471-b7730580-cec1-11ea-9488-4698c4ce286c.png" width = "600" ></p>

하지만 activation function을 ReLU로 정한 경우, 출력의 분산이 반토막 나기 때문에 이 식이 성립하지 않는다... 

weight 초기화 문제를 해결하기 위해 여러 논문이 제시 되었는데 정말 많다...
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457489-ea1cfe00-cec1-11ea-83de-2aba338556c1.png" width = "600" ></p>

## 6. Batch Normalization

만약 unit gaussian activation을 원하면 그렇게 직접 만들어보자!

현재 Batch에서 계산한 mean과 variance를 이용하여 정규화를 해주는 과정을 Model에 추가해주는 것이다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457687-6106c680-cec3-11ea-9d1b-10a6d3474dd1.png" width = "200" ></p>


각 layer에서 Weight가 지속적으로 곱해져서 생기는 Bad Scaling의 효과를 상쇄시킬 수 있다.

하지만 unit gaussian으로 바꿔주는 것이 무조건 좋은 것 인가?
이에 유연성을 붙여주기 위해 분산과 평균을 이용해 Normalized를 좀 더 유연하게 할 수 있게 했다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457744-cbb80200-cec3-11ea-80c6-ee334a0006fc.png" width = "200" ></p>

논문에 나와있는 Batch Normalization의 알고리즘은 다음과 같다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457751-ed18ee00-cec3-11ea-993b-6787ac8c29d2.png" width = "400" ></p>

Batch Normalization의 특징을 살펴보면
- Regularization의 역할도 할 수 있다.
- weight의 초기화 의존성에 대한 문제도 줄였다.
- Test할 땐 미니배치의 평균과 표준편차를 구할 수 없으니 Training하면서 구한 평균의 이동평균을 이용해 고정된 Mean과 Std를 사용한다.

Batch Normalized에 대해 설명해 놓은 블로그들도 참고하면 좋을 듯 싶다.

- [Batch Normalization 설명 및 구현](https://shuuki4.wordpress.com/2016/01/13/batch-normalization-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EA%B5%AC%ED%98%84/)

## 7. 학습 과정을 설계하는 법





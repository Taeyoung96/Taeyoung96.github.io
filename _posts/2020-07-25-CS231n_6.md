---
title : "CS231n 6강 요약"
category :
    - CS231n
tag :
    - machine_learning
    - computer vision
    - CS231n
toc : true
comments: true
---

Neural Network를 Training 시킬 때, 고려해야 하는 것들을 알아보자!

## 0. Today's Goal
> - Neural Network Traing이란?
> - Activation Function
> - Data processing
> - Weight Initialization
> - Batch Normalization
> - 학습 과정을 설계하는 법
> - Hyperparameter Optimization

## 1. 강의 및 강의 자료

강의를 들어볼 수 있는 링크와 강의 자료를 pdf형식으로 준비했다.

- 강의 링크 : [video](https://www.youtube.com/watch?v=wEoyxE0GP2M&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=6)

- 강의 슬라이드 : [슬라이드](https://github.com/Taeyoung96/Taeyoung96.github.io/files/4975049/cs231n_2017_lecture6.pdf)

한글 자막이 필요하신 분은 다음의 링크를 확인해주세요. :)

- 한글 자막을 첨부하고 싶으면 [여기](https://github.com/visionNoob/CS231N_17_KOR_SUB)를 눌러주세요.

## 2. Neural Network Traing이란?

우리가 network parameter를 최적화하는 방법 중 Gradient Descent Algorithm에 대해서 배웠다.

그리고 모든 data를 가지고 gradient descent Algorithm에 적용을 하면 계산량이 많기 때문에 SGD(Stochastic Gradient Descent) Algorithm을 이용한다.
Sample을 뽑아내 Gradient Desscent Algorithm을 사용하는 방법이다.

처음에 모델을 어떻게 선정해야 하고, Training 할 때 유의해야할 사항은 무엇인지 평가는 어떻게 해야하는지 한 번 알아보자.

## 3. Activation Function

Activation Function의 종류에 대해서 알아보자.

첫번째로 나온 함수는 Sigmoid Function이다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88452221-e5dbeb00-ce97-11ea-8991-bf79b5e400d4.png" width = "400" ></p>

이 함수는 출력이 (0,1) 사이의 값이 나오도록 하는 선형 함수이다.

하지만 단점이 존재하는데,
1. Saturated neurons가 Gradient값을 0으로 만든다.
2. 원점 중심이 아니다.
3. 지수함수가 계산량이 많다.

**Saturate**를 찾아보면 '포화'라고 해석을 하는데, 입력이 너무 작거나 클 경우 값이 변하지 않고 일정하게 1로 수렴하거나 0으로 수렴하는 것을 포화라고 생각하고 Gradient의 값이 0인 부분을 의미하는 것 같다.

그렇다면 Gradient의 값이 0이 되는 것은 왜 문제가 될까?

> Chain Rule 과정을 생각했을 때, Global gradient값이 0이 되면 즉 결과 값이 0이 되면 local gradient 값도 0이 된다. 따라서 Input에 있는 gradient 값을 구할 수 없다.

그럼 원점 중심이 아닌 것은 왜 문제가 될까?
> output의 값이 항상 양수면 다음 input으로 들어갔을 때도 항상 양수이게 된다.
그렇다면 다음 layer에서 $w$의 값을 update할 때 항상 같은 방향으로 update가 된다.
다음 그림의 예로 설명하면 우리가 원하는 vector가 파란색일 때, 
$w$의 같은 경우 제 1사분면과 제 3사분면으로 update가 되기 때문에 우리가 원하는 방향으로 update를 하기 힘들다.
> <p align="center"><img src="https://user-images.githubusercontent.com/41863759/88456683-8e9b4200-ceba-11ea-9cfd-01287b0cf132.png" width = "400" ></p>

Sigmoid의 2번 단점을 보완하기 위해 나온 function이 바로 $tanh(x)$이다.
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88456715-f487c980-ceba-11ea-8243-c2ea1d53a610.png" width = "200" ></p>

하지만 여전히 saturated한 뉴런일 때, gradient값이 0으로 된다.

따라서 새로운 activation function이 제안되었는데, 바로 **ReLU**이다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88456825-0cac1880-cebc-11ea-83b6-3e293f4cb5ea.png" width = "300" ></p>

ReLU의 특징은 (+) 영역에서 saturate하지 않고, 계산 속도도 element-wise 연산이기 때문에 sigmoid/tanh보다 훨씬 빠르다는 특징이 있다.

하지만 ReLU에도 단점은 존재하는데, 바로 (-)의 값은 0으로 만들어 버리기 때문에 Data의 절반만 activate하게 만든다는 것이다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88456895-c0150d00-cebc-11ea-894d-55484d3681ff.png" width = "500" ></p>

그래서 이러한 단점을 보완하기 위해 여러 activation function이 또 제시 되었다.

- Leaky ReLU
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88456934-3e71af00-cebd-11ea-9f84-6d607ff6ac79.png" width = "300" ></p>

- Exponential Linear Unit
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88456991-d8395c00-cebd-11ea-8748-127af7ab1f3f.png " width = "300" ></p>

Maxout이라는 activation function도 있는데, 이 함수를 사용하려면 parameter가 기존 function보다 2배 있어야 한다.

$$ max(w^T_1x + b_1, w^T_2x + b_2)$$

## 4. Data processing

데이터 전처리 과정에서는 주로 Zero-centered, Normalized, PCA,
Whitening같은 처리들을 한다.

Zero-centered 나 Normalized를 하는 이유는 모든 차원이 동일한 범위에 있어 전부 동등한 기여를 할 수 있도록 하는 것이다.

PCA나 Whitening은 더 낮은 차원으로 projection하는 느낌인데 이미지 처리에서는 이런 전처리 과정은 거치지 않는다.

기본적으로 이미지는 **Zero-centered** 과정만 거친다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457141-11be9700-cebf-11ea-8ed1-f35c5425bdc7.png" width = "500" ></p>

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457162-34e94680-cebf-11ea-853a-9f8582c74dd5.png" width = "500" ></p>

실제 모델에서는 train data에서 계산한 평균을 test data에도 동일하게 적용한다.

## 5. Weight Initialization

초기값을 몇으로 잡아야 최적의 모델을 구할 수 있을까?

만약 초기값을 0으로 한다면, 모든 뉴런은 동일한 일을 하게 될 것이다.
즉 모든 gradient의 값이 같게 될 것이다. 이렇게 하는 것은 의미가 없다.

그래서 생각한 첫번째 Idea는 '작은 random한 수로 초기화'를 하는 것이다.

초기 Weight는 표준정규분포에서 Sampling을 한다.

하지만 이런 경우 얕은 network에서는 잘 작동을 하지만 network가 깊어질 경우 문제가 생긴다.

왜냐하면 network가 깊으면 깊을수록, weight의 값이 너무 작아 0으로 수렴하기 때문이다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457360-a5dd2e00-cec0-11ea-8896-14a76d80c4f5.png" width = "600" ></p>

만약 표준 편차를 키우게 되면 어떨까?

activaton value의 값이 극단적인 값을 가지게 되고, gradient의 값이 모두 0으로 수렴할 것이다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457422-2439d000-cec1-11ea-8f98-1270679171a6.png" width = "600" ></p>

이런 초기값 문제에 대해서 'Xavier initialization'이라는 논문이 제시 되었는데
일단 activation function이 linear하다는 가정하에 다음과 같은 식을 사용하여 weight의 값을 초기화 한다.
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457443-64994e00-cec1-11ea-8861-e20e8cc8a8af.png" width = "600" ></p>

이 식을 이용하면 입/출력의 분산을 맞춰줄 수 있게 된다.
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457471-b7730580-cec1-11ea-9488-4698c4ce286c.png" width = "600" ></p>

하지만 activation function을 ReLU로 정한 경우, 출력의 분산이 반토막 나기 때문에 이 식이 성립하지 않는다... 

weight 초기화 문제를 해결하기 위해 여러 논문이 제시 되었는데 정말 많다...
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457489-ea1cfe00-cec1-11ea-83de-2aba338556c1.png" width = "600" ></p>

## 6. Batch Normalization

만약 unit gaussian activation을 원하면 그렇게 직접 만들어보자!

현재 Batch에서 계산한 mean과 variance를 이용하여 정규화를 해주는 과정을 Model에 추가해주는 것이다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457687-6106c680-cec3-11ea-9d1b-10a6d3474dd1.png" width = "200" ></p>


각 layer에서 Weight가 지속적으로 곱해져서 생기는 Bad Scaling의 효과를 상쇄시킬 수 있다.

하지만 unit gaussian으로 바꿔주는 것이 무조건 좋은 것 인가?
이에 유연성을 붙여주기 위해 분산과 평균을 이용해 Normalized를 좀 더 유연하게 할 수 있게 했다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457744-cbb80200-cec3-11ea-80c6-ee334a0006fc.png" width = "200" ></p>

논문에 나와있는 Batch Normalization의 알고리즘은 다음과 같다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88457751-ed18ee00-cec3-11ea-993b-6787ac8c29d2.png" width = "400" ></p>

Batch Normalization의 특징을 살펴보면
- Regularization의 역할도 할 수 있다.
- weight의 초기화 의존성에 대한 문제도 줄였다.
- Test할 땐 미니배치의 평균과 표준편차를 구할 수 없으니 Training하면서 구한 평균의 이동평균을 이용해 고정된 Mean과 Std를 사용한다.

### 참고가 될 만한 자료

- [Batch Normalization 설명 및 구현](https://shuuki4.wordpress.com/2016/01/13/batch-normalization-%EC%84%A4%EB%AA%85-%EB%B0%8F-%EA%B5%AC%ED%98%84/)

## 7. 학습 과정을 설계하는 법

  첫번째로 고려야 할 사항은 **데이터 전처리** 과정이다.

  두번째로는 어떤 architecture를 선택해야 하는 것인지 골라야 한다.

  그렇다면 이제 가중치가 작은 값일 때 loss값이 어떻게 분포하는지 살펴봐야 한다.

  우선 training data를 적게 잡고 loss의 값이 제대로 떨어지는지 한번 살펴보자.

여러 Hyperparameter들이 있는데 그 중 가장 먼저 고려야 해야하는 것은 **Learning rate**이다.

training 과정에서 cost가 줄어들지 않으면 Learning rate가 너무 작은지 의심을 한번 해보자. 


단, activation function이 softmax인 경우 가중치는 서서히 변하지만 accurancy값은 갑자기 증가할 수 있는데 이것은 옳은 방향으로 학습을 하고 있다는 것을 의미한다.

  <p align="center"><img src="https://user-images.githubusercontent.com/41863759/88664387-f5696700-d117-11ea-91d0-fd53b51f9b4b.png" width = "500" ></p>

  cost값이 너무 커서 발산한다면, Learning rate가 너무 큰지 의심을 한번 해보고 계속해서 값을 조정해야 한다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88664519-25b10580-d118-11ea-9727-53e1b7c2b02b.png" width = "500" ></p>

## 8. Hyperparameter Optimization

딥러닝 모델을 만들 때, 고려해야 할 Hyperparameter들이 정말 많다.

보통 training set으로 학습을 시키고 validation set으로 평가를 한다.

만약 Hyperparameter를 바꿨는데 update된 cost의 값이 원래 cost의 값보다 3배 이상 빠르게 증가할 경우 다른 parameter를 한 번 써보자.

Hyperparameter의 값을 여러 시행착오를 거쳐서 정하는 것도 하나의 방법이지만, 시간이 없다면 이러한 방법으로 hyperparameter를 찾는 것이 한계가 있다.

따라서 **Grid Search vs Random Serach** 두가지 방법이 제시되었다.

**Grid Search**는 탐색의 대상이 되는 특정 구간 내의 후보 hyperparameter 값들을 일정한 간격을 두고 선정하여, 이들 각각에 대하여 측정한 성능 결과를 기록한 뒤, 가장 높은 성능을 발휘했던 hyperparameter 값을 선정하는 방법이다.

반면 **Random Search**는 Grid Search와 큰 맥락은 유사하나, 탐색 대상 구간 내의 후보 hyperparameter 값들을 랜덤 샘플링(sampling)을 통해 선정한다는 점이 다르다. Random Search는 Grid Search에 비해 불필요한 반복 수행 횟수를 대폭 줄이면서, 동시에 정해진 간격(grid) 사이에 위치한 값들에 대해서도 확률적으로 탐색이 가능하므로, 최적 hyperparameter 값을 더 빨리 찾을 수 있는 것으로 알려져 있다.

따라서 실제로는 random search가 더 좋은 방법이라고 알려져 있다.

### 참고가 될 만한 자료

- [딥러닝 모델의 효과적인 hyperparameter 탐색 방법론 (1)](http://research.sualab.com/introduction/practice/2019/02/19/bayesian-optimization-overview-1.html)
- [딥러닝 모델의 효과적인 hyperparameter 탐색 방법론 (2)](http://research.sualab.com/introduction/practice/2019/04/01/bayesian-optimization-overview-2.html)

Hyperparameter를 정할 때 loss curve를 보고 이 hyperparameter가 적합한지 아닌지 평가를 하는 경우가 많다.

만약 loss curve가 초기에 평평하다면 초기화가 잘못될 가능성이 클 것이다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88666195-b7217700-d11a-11ea-9965-3e002ab76bd3.png" width = "400" ></p>

그리고 training accuracy와 validation accuracy가 gap이 클 경우 overfitting이 된 가능성이 매우 높은 것이다.

그 gap이 없을 경우 model capacity를 늘리는 것을 고려해봐야 한다.
즉, training한 dataset이 너무 작은 경우일 수도 있다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88666895-a02f5480-d11b-11ea-9f43-87929588e22f.png" width = "500" ></p>


---

이렇게 Training Neural Networks 중 Activation Function, 데이터 전처리, Weight 초기화, Batch Normalization, Hyperparameter Optimization하는 경우를 간단히 살펴봤다.

다음 시간에 이어서 나머지 부분을 배워보자.


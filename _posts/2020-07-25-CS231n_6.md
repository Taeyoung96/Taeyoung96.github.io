---
title : "CS231n 6강 요약"
category :
    - CS231n
tag :
    - machine_learning
    - computer vision
    - CS231n
toc : true
comments: true
---

Neural Network를 Training 시킬 때, 고려해야 하는 것들을 알아보자!

## 0. Today's Goal
> - Neural Network Traing이란?
> - Activation Function
> - Data processing
> - Weight Initialization
> - Batch Normalization
> - 학습 과정을 설계하는 법
> - Hyperparameter Optimization

## 1. 강의 및 강의 자료

강의를 들어볼 수 있는 링크와 강의 자료를 pdf형식으로 준비했다.

- 강의 링크 : [video](https://www.youtube.com/watch?v=wEoyxE0GP2M&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=6)

- 강의 슬라이드 : [슬라이드](https://github.com/Taeyoung96/Taeyoung96.github.io/files/4975049/cs231n_2017_lecture6.pdf)

한글 자막이 필요하신 분은 다음의 링크를 확인해주세요. :)

- 한글 자막을 첨부하고 싶으면 [여기](https://github.com/visionNoob/CS231N_17_KOR_SUB)를 눌러주세요.

## 2. Neural Network Traing이란?

우리가 network parameter를 최적화하는 방법 중 Gradient Descent Algorithm에 대해서 배웠다.

그리고 모든 data를 가지고 gradient descent Algorithm에 적용을 하면 계산량이 많기 때문에 SGD(Stochastic Gradient Descent) Algorithm을 이용한다.
Sample을 뽑아내 Gradient Desscent Algorithm을 사용하는 방법이다.

처음에 모델을 어떻게 선정해야 하고, Training 할 때 유의해야할 사항은 무엇인지 평가는 어떻게 해야하는지 한 번 알아보자.

## 3. Activation Function

Activation Function의 종류에 대해서 알아보자.

첫번째로 나온 함수는 Sigmoid Function이다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/88452221-e5dbeb00-ce97-11ea-8991-bf79b5e400d4.png" width = "400" ></p>

이 함수는 출력이 (0,1) 사이의 값이 나오도록 하는 선형 함수이다.

하지만 단점이 존재하는데,
1. Saturated neurons가 Gradient값을 0으로 만든다.
2. 원점 중심이 아니다.
3. 지수함수가 계산량이 많다.

**Saturate**를 찾아보면 '포화'라고 해석을 하는데, 입력이 너무 작거나 클 경우 값이 변하지 않고 일정하게 1로 수렴하거나 0으로 수렴하는 것을  


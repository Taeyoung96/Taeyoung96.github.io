---
title : "CS231n 4강 요약"
category :
    - CS231n
tag :
    - machine_learning
    - computer vision
    - CS231n
toc : true
toc_sticky: true
comments: true
---

오늘은 Backpropagation과 Neural Networks에 대해 배워보자!

## 0. Today's Goal
> - Backpropagation에 대해서 알아보자.
> - 간단한 용어 정리
> - Neural Networks란 무엇일까?

## 1. 강의 및 강의 자료

강의를 들어볼 수 있는 링크와 강의 자료를 pdf형식으로 준비했다.

- 강의 링크 : [video](https://www.youtube.com/watch?v=d14TUNcbn1k)

- 강의 슬라이드 : [슬라이드](https://github.com/Taeyoung96/Taeyoung96.github.io/files/4929628/cs231n_2017_lecture4.pdf)

한글 자막이 필요하신 분은 다음의 링크를 확인해주세요. :)

- 한글 자막을 첨부하고 싶으면 [여기](https://github.com/visionNoob/CS231N_17_KOR_SUB)를 눌러주세요.

## 2. 강의 요약

### Backpropagation에 대해서

우리가 optimization 과정을 거칠 때, 가장 loss가 낮은 곳을 찾기 위해서,
Gradient descent algorithm을 사용한다. 이 때 출력 값과 그 gradient(기울기)를 이용하여 input에서의 gradient값을 구하고 싶을 때 사용하는 방법이 **backpropagation**이다.

이 방법을 사용하기 위해 우리는 **Computational graphs**를 사용한다.

<U>Computational graphs는 함수 식이 있을 때, 그것을 단순화하기 위해 graph로 표현하여 함수를 시각화하는 것</U>을 말한다.

가장 간단한 예로 $f(x,y,z) = (x+y)z$라는 함수가 있다고 하자.

$x = -2, y = 5, z = 4$라고 값이 주어지고

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/87634652-aa933b00-c778-11ea-9896-9e5719771d9f.png" width = "300" ></p>

만약 여기서 우리가 $\frac {\partial f} {\partial x} , \frac {\partial f} {\partial y}, \frac {\partial f} {\partial z}$의 값을 구하려면 어떻게 해야할까?


이를 computational graph로 표현하면 다음과 같다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/87634228-d4982d80-c777-11ea-8429-61b9ec2c53a5.png" width = "400" ></p>

여기서 우리는 **Chain rule**이라는 개념을 사용하여 이 문제를 풀 수 있다.

**Chain rule**이란 <U>연쇄 법칙이라고도 하는데 합성함수의 미분</U>을 생각하면 이해하기 쉽다. 

이 예시에서는 $\frac {\partial f} {\partial z} $를 chain rule을 활용하여

$\frac {\partial f} {\partial z} = \frac {\partial f} {\partial q} \frac {\partial q} {\partial z} $로 표현하여 간단히 값을 구할 수 있다.

**Chain rule**의 핵심은 <U>그 전 단계의 gradient값을 구할 때 결과값의 gradient와 변수 사이의 관계를 이용</U>하여 구할 수 있다는 점이다.

즉, 인접한 노드간의 정보들을 알고 있고, 결과 값의 gradient를 알고 있으면 이전 노드의 gradient를 구할 수 있다.

강의에서는 local gradient와 upstream gradient의 곱을 이용하여 구할 수 있다고 한다.

우리는 이것을 한층 확장하여 vector에 대해서도 이를 생각해 볼 수 있는데,

예시를 들어 알아보자.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/87759059-9d458180-c848-11ea-8042-4c758b8ced8a.png" width = "600" ></p>

강의 슬라이드에 보면 굉장히 괴상한(?) 문자들이 많이 나와있다..

하지만 결국 하고 싶은 말은 <U>인접한 노드와의 관계를 이용해서 gradient값을 얻어내는 것</U>을 말하고 있다.

여기서 jacobian matrix에 대한 내용도 나오는데, 쉽게 말하면 다변수 벡터 함수의 도함수 행렬이다.  
모든 함수의 편미분 값들을 행렬로 표현했다고 생각하자. (난 그렇게 이해를 했다. :))

구해야하는 gradient의 차원이 늘어난 것이므로 행렬식으로 표현한 것이라고 생각하면 이해하기 쉬울 것이다.

### 간단한 용어 정리

> - forward : backpropagation 과정에서 gradient를 순서대로 계산을 하고 그 단계별 결과 값을 저장하는 과정들
> - backward : chain rule을 적용하여 loss function의 gradient 값(최종 결과 값)을 이용하여 input의 관점에서의 gradient를 구하는 과정들
> - chain rule : gradient를 구할 때, 두 변수 사이의 관계를 이용하여 연쇄 법칙을 적용하는 것
> - jacobian matrix : 다변수 함수가 있을 때 그 변수들에 대한 편미분값을 행렬로 표현한 것


### Neural Networks에 대해서

우리는 지금까지 주로 Linear score function에 대해서 배웠다.

예를 들어 $f = Wx$같은 간단한 function들..

이러한 Linear function과 비선형 변환을 겹겹이 쌓아간다면 우리는 더 복잡한 문제를 해결할 수 있는 모델을 가질 수 있을 것이다.

예를 들어 $f = W_2 max(0,W_1x)$ 같은 함수들로는 좀 더 복잡한 문제를 해결할 수 있다.

우리는 이런 모델을 신경망 구조에서 아이디어를 얻어 복잡한 모델을 만든다.

일대일 대응까지는 아니지만 그래도 많은 부분이 비슷하다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/87761934-3fffff00-c84d-11ea-9b22-7a76bb6e7ade.png" width = "700" ></p>

강의에서 비선형 함수로는 여러가지가 있다고 소개를 하고 
자세한 설명은 다음 시간에 한다고 하고 넘어갔다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/87763227-6757cb80-c84f-11ea-9e35-e287acb35434.png
" width = "500" ></p>

보통 Sigmoid function과 ReLU function을 많이 사용한다고 한다.

뉴런 구조를 이해할 때 주의할 점!
> - 정말 다양한 구조가 있다.
> - Dendrite가 linear한 계산이 아닌 non-linear computations도 수행될 수 있다.
> - Synapses는 하나의 weight가 아니라 복잡한 dynamical한 system일 수 있다.
> - 우리가 activation function을 통해서 얻은 rate들이 적절하지 않을 수 있다.

**즉, 뉴런 구조는 우리가 생각한것 보다 더 복잡한 구조이므로, 이를 단순화해서 생각하는 것이지 완벽하게 일대일 대응은 아니다!**

Neural network의 architecture는 다음과 같다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/87763153-468f7600-c84f-11ea-86d9-117c285d6569.png
" width = "500" ></p>

보통 "3-layer Neural Net" 또는 "2-hidden-layer Neural Net"이라고 한다.

모든 layer들이 하나도 빠짐없이 다 연결되어 있는데 이를 **Fully-connected layers**라고 한다.

다음 시간에는 convolution neural networks를 한번 배워보자!












---
title : "CS231n 4강 요약"
category :
    - CS231n
tag :
    - machine_learning
    - computer vision
    - CS231n
toc : true
comments: true
---

오늘은 Backpropagation과 Neural Networks에 대해 배워보자!

## 0. Today's Goal
> - Backpropagation에 대해서 알아보자.
> - 간단한 용어 정리
> - Neural Networks란 무엇일까?

## 1. 강의 및 강의 자료

강의를 들어볼 수 있는 링크와 강의 자료를 pdf형식으로 준비했다.

- 강의 링크 : [video](https://www.youtube.com/watch?v=d14TUNcbn1k)

- 강의 슬라이드 : [슬라이드](https://github.com/Taeyoung96/Taeyoung96.github.io/files/4929628/cs231n_2017_lecture4.pdf)

한글 자막이 필요하신 분은 다음의 링크를 확인해주세요. :)

- 한글 자막을 첨부하고 싶으면 [여기](https://github.com/visionNoob/CS231N_17_KOR_SUB)를 눌러주세요.

## 2. 강의 요약

### Backpropagation에 대해서

우리가 optimization 과정을 거칠 때, 가장 loss가 낮은 곳을 찾기 위해서,
Gradient descent algorithm을 사용한다. 이 때 출력 값과 그 gradient(기울기)를 이용하여 input에서의 gradient값을 구하고 싶을 때 사용하는 방법이 **backpropagation**이다.

이 방법을 사용하기 위해 우리는 **Computational graphs**를 사용한다.

Computational graphs는 함수 식이 있을 때, 그것을 단순화하기 위해 graph로 표현하여 함수를 시각화하는 것을 말한다.

가장 간단한 예로 $f(x,y,z) = (x+y)z$라는 함수가 있다고 하자.

$x = -2, y = 5, z = 4$라고 값이 주어지고

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/87634652-aa933b00-c778-11ea-9896-9e5719771d9f.png" width = "300" ></p>

만약 여기서 우리가 $\frac {\partial f} {\partial x} , \frac {\partial f} {\partial y}, \frac {\partial f} {\partial z}$의 값을 구하려면 어떻게 해야할까?


이를 computational graph로 표현하면 다음과 같다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/87634228-d4982d80-c777-11ea-8429-61b9ec2c53a5.png" width = "400" ></p>

여기서 우리는 **Chain rule**이라는 개념을 사용하여 이 문제를 풀 수 있다.

**Chain rule**이란 연쇄 법칙이라고도 하는데 합성함수의 미분을 생각하면 이해하기 쉽다. 

이 예시에서는 $\frac {\partial f} {\partial z} $를 chain rule을 활용하여

$\frac {\partial f} {\partial z} = \frac {\partial f} {\partial q} \frac {\partial q} {\partial z} $로 표현하여 간단히 값을 구할 수 있다.

**Chain rule**의 핵심은 그 전 단계의 gradient값을 구할 때 결과값의 gradient와 변수 사이의 관계를 이용하여 구할 수 있다는 점이다.

즉, 인접한 노드간의 정보들을 알고 있고, 결과 값의 gradient를 알고 있으면 이전 노드의 gradient를 구할 수 있다.

강의에서는 local gradient와 upstream gradient의 곱을 이용하여 구할 수 있다고 한다.

우리는 이것을 한층 확장하여 vector에 대해서도 이를 생각해 볼 수 있는데,

예시를 들어 알아보자.








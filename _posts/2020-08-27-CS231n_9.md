---
title : "CS231n 9강 요약"
category :
    - CS231n
tag :
    - machine_learning
    - computer vision
    - CS231n
toc : true
comments: true
---

대표적인 CNN Architecture에는 어떤 것들이 있는지 알아보자!

## 0. Today's Goal
> - AlexNet
> - VGG
> - GooLeNet
> - ResNet
> - 그 밖에 다양한 모델 소개

## 1. 강의 및 강의 자료

강의를 들어볼 수 있는 링크와 강의 자료를 pdf형식으로 준비했다.

- 강의 링크 : [video](https://www.youtube.com/watch?v=DAOcjicFr1Y&list=PL3FW7Lu3i5JvHM8ljYj-zLfQRF3EO8sYv&index=9)

- 강의 슬라이드 : [슬라이드](https://github.com/Taeyoung96/Taeyoung96.github.io/files/5135217/cs231n_2017_lecture9.pdf)

한글 자막이 필요하신 분은 다음의 링크를 확인해주세요. :)

- 한글 자막을 첨부하고 싶으면 [여기](https://github.com/visionNoob/CS231N_17_KOR_SUB)를 눌러주세요.

## 2. AlexNet

CNN이 주목을 받기 시작하게 된 계기는 AlexNet 덕분이라고 해도 과언이 아니다.  

이 강의에서는 각 모델에서 필요한 Meomory와 Parameter의 수를 중점적으로 설명을 한다.

AlexNet을 도식화 하면 다음과 같다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91433958-105a0300-e89f-11ea-95ff-d6e63acb3430.png" width = "500" ></p>

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91434932-7eeb9080-e8a0-11ea-99e9-c9a79ad70b17.png" width = "400" ></p>

이 논문을 2012년에 발표를 했었는데, 그 당시 컴퓨터 성능이 좋지 않아, 네트워크를 분산시켜 GPU에 넣었다.  
그래서 feature map을 추출할 때 2개의 영역으로 나누어져 있다.


AlexNet의 특징들에 대해서 살펴보면
> - ReLU를 처음으로 사용
> - Norm Layer를 사용
> - Data Augmentation을 많이 사용
> - Dropout : 0.5
> - Batch size : 128
> - SGD Momentum : 0.9
> - Learning rate : 1e-2
> - L2 weight decay : 5e-4
> - 7 CNN ensemble : 18.2% -> 15.4%

## 3. VGG

ILSVRC 우승 모델 중 다음으로 알아볼 모델은 **VGG**모델이다.  
AlexNet과 비교를 했을 때, Layer의 수가 확실히 깊어진 것을 알 수 있다.  

3x3 convolution layer 필터만을 사용하여 좀 더 깊은 Network를 가질 수 있도록 설계하였다.  
뿐만 아니라 3x3 convolution layer 필터를 사용함으로써 파라미터의 수도 더 적게 되었다. 

VGG 모델은 VGG16, VGG19가 대표적인데 Layer의 갯수에 따라 다른 이름이 붙어졌다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91437264-9298f600-e8a4-11ea-88fb-3595bb8fbc5a.png" width = "400" ></p>

전체적으로 각 Layer마다 필요한 memory와 parameter의 수를 정리하면 다음과 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91437358-bceab380-e8a4-11ea-9d84-dd2ebc52901a.png" width = "600" ></p>

VGG의 특징들에 대해서 살펴보면
> - AlexNet과 비슷한 절차로 Training을 거침
> - Local Response Normalisation(LRN)이 없음
> - Ensemble을 사용하여 최고의 결과를 뽑아냄
> - 맨 마지막에서 두번째 FC (fc7)은 다른 task들을 가지고 잘 일반화를 함.

## 4. GoogLeNet

그 다음 주목을 받은 CNN 구조는 **GoogLeNet**이다.

모델의 구조는 다음과 같다.


<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91437837-76498900-e8a5-11ea-8720-3155c9c517da.png" width = "200" ></p>

한 눈에 봐도 굉장히 복잡한 구조로 이루어져 있음을 알 수 있다.

가장 큰 특징으로는 **Inception module**을 사용했다는 것이다.

간단히 네트워크 안에 작은 네트워크를 구성하였다고 생각하자.

GoogLeNet의 특징들에 대해서 살펴보면
> - 22 Layer들로 구성
> - 파라미터의 수를 줄이기 위해 Fully Connected Layer를 없앰.
> - 총 5백만 개의 파라미터 수 (AlexNet과 비교하면 현저히 줄어듬)
> - 높은 계산량을 효율적으로 수행

**Inception module**에 대해 좀 더 자세히 살펴보면

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91441946-10143480-e8ac-11ea-8cc3-e766bee4f01d.png" width = "400" ></p>

동일한 입력을 받는 서로 작은 다양한 필터들이 병렬적으로 존재를 한다

하지만 이러한 구조로 Inception module을 만들면 한가지 문제점이 발생하는데,  

바로 어마어마한 계산량이다...

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91442357-b2341c80-e8ac-11ea-8eea-4e99cd24d710.png
" width = "500" ></p>

이러한 계산량을 줄이기 위해서 1x1 convolution layer를 사용한다.
이렇게 되면 Input의 depth가 줄어드는 효과가 난다.
이것을 **Bottleneck layer**라고 한다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91444522-d34a3c80-e8af-11ea-8c04-286794cc62b1.png
" width = "500" ></p>

1x1 convolution layer로 depth를 줄이면 정보의 손실이 있지만 동작을 더 잘한다.

차원을 줄인 후 parameter의 수를 다시 조사해보면

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91444716-1c01f580-e8b0-11ea-819c-c0037498d48f.png
" width = "500" ></p>

parameter의 수가 확실히 떨어진 것을 알 수 있다.

다시 한번 GooLeNet의 Architecture를 살펴보면,

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91445250-da257f00-e8b0-11ea-8f18-ac326b8973ac.png
" width = "500" ></p>

이렇게 보조 분류기가 있는 것을 확인 할 수 있는데,  
이유는 네트워크의 깊이가 깊기 때문에 중간 Layer의 학습을 돕기 위해서 설계해 놓은 것이다.  

## 5. ResNet

이번에는 ResNet이라는 녀석이다.  
Residual connection이라는 새로운 구조로 굉장한 성능을 보였다.

기본적인 Residual block의 모습과 모델의 Architecture는 다음과 같다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91446079-eeb64700-e8b1-11ea-860e-b8f0da187633.png
" width = "500" ></p>

일반적으로 CNN을 깊게 쌓게 되면 어떤 일이 발생할까?  
더 깊은 layer를 쌓게 되면 Overfitting이 일어나지 않을까?
하지만 결과는 그렇지 않았다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91446435-73a16080-e8b2-11ea-938a-20e789ec78dd.png
" width = "600" ></p>


더 깊은 layer를 가진 구조가 training error가 더 높았다.  
만약 Overfitting이 되었더라면 training error는 더 낮아야 한다.  

여기서 한 가지 가설을 했는데,  
더 깊은 모델은 Optimize하는데 훨씬 더 어렵다는 가설이다.  

즉, 네트워크 구조가 깊으면 깊을 수록 어느 순간 그 모델은 얕은 모델보다 더 train이 안된다는 것인데 이러한 문제를 Degradation이라고 한다.

이 가설에 대한 해결책으로는 다음과 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91447354-89fbec00-e8b3-11ea-8837-f3ef2d52ed37.png
" width = "600" ></p>

일반적으로 Layer를 쌓아 올리는 방식 대신  
Skip connection이라는 새로운 구조를 이용하여 학습을 진행한다.

가중치 Layer는 $H(x) - x$에 대한 값이 0에 수렴하도록 학습을 진행한다.  

그 결과, 연산 증가도 크게 없고 만약 Gradient Vaninshing 현상이 일어나더라도 원본 신호에 대한 내용을 가지고 있어서 학습을 하는데 원할하게 시킬 수 있다는 것이 ResNet을 만든 연구진들이 하는 설명이다. 

이러한 Residual block을 사용한 결과 네트워크의 깊이가 깊어질 수록 더 정확하게 training을 시킬 수 있었다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91449430-0e4f6e80-e8b6-11ea-9612-35cb8c9d3dc3.png
" width = "600" ></p>










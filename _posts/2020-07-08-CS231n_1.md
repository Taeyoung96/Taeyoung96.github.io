---
title : "CS231n 1강 요약"
category :
    - CS231n
tag :
    - machine_learning
    - CS231n
    - computer vision
toc : true
comments: true
---

CS231n은 Stanford 강좌 중 하나이다.

이 강의에서는 CNN에 관한 전반적인 내용을 배운다.

첫번째 강의를 요약하고 정리를 해보았다.

주관적인 입장에서 정리를 한 것이라 오류가 있을 수 있다.

## 0. Today's Goal
> - Computer Vision의 역사에 대해 알아보기
> - CS231n 강의를 통해 배울 수 있는 것

## 1. 강의 및 강의 자료

강의는 유튜브에 올라와 있고, 강의 자료도 홈페이지에 나와있다.

친절하게 한글 자막까지 만드신 분도 있다..

- 강의 링크 : [video](https://www.youtube.com/watch?v=vT1JzLTH4G4&t=0s&index=1&list=PLC1qU-LWwrF64f4QKQT-Vg5Wr4qEE1Zxk)

- 강의 슬라이드 : [슬라이드](https://github.com/Taeyoung96/Taeyoung96.github.io/files/4889161/cs231n_2017_lecture1.pdf)

- 한글 자막을 첨부하고 싶으면 [여기](https://github.com/visionNoob/CS231N_17_KOR_SUB)를 눌러주세요.

## 2. 강의 요약

여기서 사용하는 그림은 대부분 강의자료에 있는 자료들이다.

먼저 Computer Vision은 굉장히 여러 분야를 알아야 하는 과목이다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/86892290-06d3da80-c13b-11ea-9c9f-914fcee406e9.png" width = "400" ></p>

우리 인간의 시각체계는 매우 복잡하고 중요한 감각처리인데, 이에 대한 연구는 굉장히 오래 전부터 진행되어 왔다.

Computer Vision 분야에서 굉장히 큰 영향을 준 연구들을 소개하자면,

1959년, Hubel & Wiesel 박사님들은 포유류의 시각 메커니즘은 어떻게 될것인가에 대해 연구했다.

그리고 1963년 Larry Roberts 박사님은 우리가 보는 사진을 특징점으로 재구성하는 방법에 대해 연구했다.

1970년대 David Marr 박사님은 Vision 이라는 책을 통해 Computer Vision이 어떤 방식으로 발전해야 하는지 제시를 했다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/86892896-d6407080-c13b-11ea-80ac-46e74d2ab33f.png" width = "400" ></p>

Input Image가 들어왔을 때 이미지의 특징들을 추출하고, 특징에 따라 depth나 surface를 추출한다. 이렇게 추출한 정보를 가지고 3D modeling 하는 방법을 제시했다.

Object Detection 분야는 굉장히 어렵기 때문에 객체 분할을 먼저 시도하는 연구도 있다.(Object segmentation)

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/86893675-f58bcd80-c13c-11ea-9ffd-468656b9ee18.png" width = "400" ></p>

시간이 흘러, 90년도 후반에서 2010년도 까지는 특징을 기반으로 한 객체 인식 알고리즘이 대세를 이루었다.

이런 대세의 흐름에 영향을 준 논문이 바로 SIFT 알고리즘이다.

객체의 모양은 카메라의 각도에 따라, 조도의 영향에 따라 달라질 수 있지만 불변하는 특징을 찾아 특징점끼리 매칭을 하는 것이 이 알고리즘의 main idea이다.
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/86893967-59ae9180-c13d-11ea-9ec0-eed3a433eb2d.png" width = "400" ></p>

그 이후 Spatial Pyramid Matching, HOG algorithm 등등 여러 논문이 Computer Vision 분야에 큰 영향을 끼쳤다.

2000년대 초에 Computer Vision분야에서 앞으로 풀어야 할 문제가 무엇인지 정의했다.

SVM, Adaboost 같은 Machine Learning 알고리즘을 Training 하는 과정에서 Onverfit가 일어나는 문제가 생기는데 이러한 이유는 2가지가 있다.

> 1. 시각 데이터가 매우 복잡하기 때문이다.
> 2. 학습 데이터가 부족하기 때문이다.

따라서 Object detection 분야에서의 두 가지 목표를 세운다.
> 1. 세상의 모든 객체를 인식한다.
> 2. Overfitting을 줄여보자.

이 목표들을 가지고 엄청난 데이터셋을 만들게 된다.
바로 **ImageNet**이다.  

**ImageNet**은 22K 의 Categories와 14M 개의 이미지 사진이 들어있는 엄청난 Dataset이다.

이 Dataset을 가지고 Image Classification을 하는 대회를 열게 되는데,  
그것이 바로 **ILSVRC(ImageNet Large Scale Visual Recognition Challenge)** 이다. 
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/86897624-a5b00500-c142-11ea-9972-842272663963.png" width = "400" ></p>

위의 슬라이드를 살펴보면 2012년 오차율이 10%로 줄어든 혁신적인 알고리즘이 제안되었다. 바로 **CNN 구조를 기반으로 한 ALexNet**이다.

그 이후 모두 CNN을 변형해서 성능을 좋게 하려는 연구가 계속해서 일어나고 있다.

따라서 CS231n 수업을 통해 **<U>CNN이 무엇인지 배우게 될 것이다.</U>**

그렇다면 CNN 구조는 갑자기 '툭' 튀어나온 알고리즘인가?

그렇지 않다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/86898115-40a8df00-c143-11ea-995e-f1164d33bb59.png" width = "500" ></p>

1998년에 이미 CNN 알고리즘이 제안되었다.  
하지만 여러 문제들에 부딫혀 그 동안 발전하지 못했다.

> - 데이터의 양이 너무 적었다.
> - 연산 속도가 굉장히 느렸다.

최근 여러 기술들이 발전하면서 한계점들을 극복하고 있다.

그럼 다음 강의부터 본격적으로 CNN에 대해 배워보자!









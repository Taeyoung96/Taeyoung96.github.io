---
title : "CS231n 11강 요약"
category :
    - CS231n
tag :
    - machine_learning
    - computer vision
    - CS231n
toc : true
comments: true
---

Object Detection과 Segmentation에 대해서 알아보자!

## 0. Today's Goal
> - Semantic Segmentation
> - Instance Segmentation
> - Classification + Localization
> - Object Detection


## 1. 강의 및 강의 자료

강의를 들어볼 수 있는 링크와 강의 자료를 pdf형식으로 준비했다.

- 강의 링크 : [video](https://www.youtube.com/watch?v=nDPWywWRIRo&t=6s)

- 강의 슬라이드 : [슬라이드](https://github.com/Taeyoung96/Taeyoung96.github.io/files/5139557/cs231n_2017_lecture11.pdf)

한글 자막이 필요하신 분은 다음의 링크를 확인해주세요. :)

- 한글 자막을 첨부하고 싶으면 [여기](https://github.com/visionNoob/CS231N_17_KOR_SUB)를 눌러주세요.

## 2. Semantic Segmentation

지금까지 CS231n 강의는 주로 Image Classification에 대해서 다루었다.  

하지만 Computer Vision 분야에는 Image Classification 말고도 여러 Task들이 있는데 그 중 먼저 알아볼 것이 **Semantic Segmentation**이다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91511786-77b59880-e91b-11ea-8c09-3b9282c81a61.png" width = "500" ></p>

**Semantic Segmentation**은 이미지의 픽셀들이 어떤 클래스에  속하는지 예측하는 과정이다.  
Classification과 마찬가지로 미리 클래스의 수와 종류를 정해 놓아야 한다.  
하지만 Semantic Segmentation은 개별 객체가 구분되지 않는 단점이 있다.  
(윗 그림의 소의 예를 참고하자.)    

그렇다면 Semantic Segmentation을 구현하기 위해서는 어떻게 해야할까?  
가장 기본적으로 생각할 수 있는 IDEA는 Sliding Window기법이다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91512280-a08a5d80-e91c-11ea-9b1f-ef8a6ce26dfa.png" width = "500" ></p>

Patch를 만들고, 전체 영역을 모두 돌면서 한 픽셀이 어느 카테고리에 들어가는지 찾아내는 방법이다.  
하지만 잠깐만 생각해봐도 굉장히 계산량이 어마어마 할 것이라는 것을 알 수 있다.  

두번째로 제안된 방법은 Fully Convolutional Network를 이용한 방법이다. 

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91512682-800ed300-e91d-11ea-9c8c-e72679da931b.png" width = "600" ></p>

입력의 Spatial Size (공간 정보)를 유지해야 한다는 특징이 있다.  
이것 또한 모든 픽셀에 대해 카테고리 Score를 계산해야 하기 때문에 계산량이 어마어마하다.

따라서 나온 방법이 Fully Connected Layer 대신 spatial resolution을 다시 키우는 방법이 제안되었다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91512987-34a8f480-e91e-11ea-8436-d9a50928b49f.png" width = "600" ></p>

이 과정에서 Upsampling이라는 과정을 거치는데 이 강의에서 처음 등장한 용어이다.  
예시를 들어 좀 더 자세히 설명을 해보겠다.  

### Upsampling

Upsampling은 간단히 생각하면 Downsampling의 역과정이라고 생각하면 된다.  
이미지의 사이즈를 늘리게 되면 빈 공간이 생겨날텐데, 이런 빈 공간을 어떻게 채우는지 Issue가 발생한다.  

이 강의에서 설명한 방법은 크게 3가지이다.
> 1. Unpooling
> 2. Max Unpooling
> 3. Transpose Convolution

- Unpooling
빈공간을 어떻게 채우는지 Issue가 생겨난다고 했는데,  
단순히 0으로 채우거나 인접한 값을 복사하여 채우는 방법이 있다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91513342-0d065c00-e91f-11ea-8db2-73e055a0d9fa.png" width = "500" ></p>

- Max Unpooling
대부분의 네트워크가 downsampling 과정을 거칠 때 Maxpooling 과정을 거치는데 네트워크가 주로 대칭적인 특징을 가지고 있어 그 위치로 Upsampling을 하는 과정이다.  
하지만 특정 Feature Map의 공간성을 잃는다는 단점이 있다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91513584-adf51700-e91f-11ea-9797-eb992f223d71.png" width = "500" ></p>

- Transpose Convolution

위의 두 방법은 단어를 보고 직관적으로 이해가 되었지만,  
이번 방법은 단어만 보고는 잘 와닿지 않는다.  

Transpose Convolution은 Convolution 연산을 행렬로 생각하고 내적의 과정으로 이해하면 이해하기 편하다.  

예를 들어, 3x3 transpose convolution, stride 2, pad 1을 한다고 할 때,  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91513823-37a4e480-e920-11ea-86f1-d6b4d1bdd111.png" width = "400" ></p>

입력값을 가중치라고 생각하고, 필터의 값을 곱해서 output에 채워넣는다고 이해하자.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91513892-63c06580-e920-11ea-8b18-807fa6c1ee77.png" width = "400" ></p>

만약 겹치는 부분이 있을 경우 그 구간에 있는 값들을 모두 더해주면 된다.  
평균을 내지 않고 왜 더하지라는 궁금증이 있었는데, 그냥 공식처럼 쓰인다고 한다.  

Transpose Convolutioin을 처음 설명할 때 행렬로 이해하면 쉽다고 했는데  
1차원 행렬을 가지고 예를 들어 이해해보자.  

필터 : $\vec{x} = (x,y,z)$   ,입력 : $\vec{a} = (a,b,c,d)$라고 하고 convolution을 진행하면

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91514335-78e9c400-e921-11ea-86b4-708a32889229.png" width = "300" ></p>

이제 Transpose convolution을 진행해보자.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91514631-31176c80-e922-11ea-9bdf-a2333492f2d0.png" width = "500" ></p>

stride가 1일 때는 둘의 연산의 크게 차이가 없다는 것을 확인할 수 있다.

하지만 stride가 2일 때는 다르다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91514919-efd38c80-e922-11ea-8d7c-0c09c368d792.png" width = "500" ></p>

결과의 값이 입력의 값보다 크게 확장되었다는 느낌을 받을 수 있다.

이래서 Upsampling하는데 사용을 하는 것 같다.  

이에 대한 동의어로는 다음과 같다.
- Deconvolution
- Upconvolution
- Fractionally strided convolution
- Backward strided convolution

## 3. Instance Segmentation

앞서 살펴본 Semantic Segmentation은 개별 객체를 구별할 수 없다는 단점이 있었다.  
이 단점을 보완하기 위해 나온 방법이 Instance Segmentation이다.  

각각의 픽셀별로 카테고리 score를 매기는 방식이 아니라,  
각각 픽셀별로 Object가 있는지 없는지를 판단한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91515561-6c1a9f80-e924-11ea-87a4-f6f4e076684b.png" width = "500" ></p>

Instance Segmentation과 Object Detection을 결합하여 만들어진 대표적인 알고리즘이 Mask R-CNN이다.  

Mask R-CNN에 대한 설명은 Object Detection에 대한 설명이 끝나면 간단히 해보겠다.

## 4. Classification + Localization

Classification과 Localization은 이미지에서 하나의 물체에 대해 그 물체가 어떤 것에 속하고, 그 물체가 어디에 위치해 있는지 찾아내는 Task이다.  

보통 Image Classification을 위한 네트워크는 최종적으로 하나의 FC layer가 존재하지만  
이 Task는 2개의 FC layer를 이용하여 Bounding Box와 Classification을 찾는다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91516213-d8e26980-e925-11ea-9200-d62395b2253b.png" width = "500" ></p>

따라서 학습을 시킬 때 2개의 Loss function을 이용한다.

여기서 2개의 Loss function의 단위가 달라서 gradient 계산이 되는지 의문이 들 수 있다.  

하지만 Multitask Loss이기 때문에 네트워크 가중치들의 각각의 미분 값을 계산하므로 상관이 없다.  

이러한 방식으로 Human Pose Estimation에도 적용을 할 수 있다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91522016-1f3ec500-e934-11ea-8278-31b9c772cac9.png" width = "500" ></p>

## 5. Object Detection

Object Detection은 한 장의 이미지에서 다수의 물체를 찾고, 그 물체가 어디 있는지 알아내는 Task이다.  

Classification + Localization과 다른 점은 이미지 마다 객체의 수가 달라져서 이를 미리 예측할 수 없다는 것이다.

즉, 더 어려운 Task이다.

Object Detection은 크게 **2-stage Detector**와 **1-stage Detector**로 나눠질 수 있는데  
이를 나누는 기준은 Classification과 Localization을 단계별로 할 경우 **2-stage Detector**라 하고, 이를 한번에 진행할 경우  **1-stage Detector**라고 한다.  

먼저 2-stage Detector에는 크게 3가지의 알고리즘에 대해서 다룰 것이다.
> - R-CNN
> - Fast R-CNN
> - Faster R-CNN

### R-CNN

R-CNN에 대해서 소개하기 전에 먼저 알아야 할 것이 바로 **Selective Search  Algorithm**이다.

딥러닝을 쓰지않고 Rule based로 된 알고리즘으로 물체가 있는 영역을 찾는데 효과적인 알고리즘이다.  
즉, Region Proposals을 딥러닝을 쓰지않고 찾는 방법 중 가장 성능이 좋은 알고리즘이다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91522441-316d3300-e935-11ea-9dd7-ea59deb8f28a.png" width = "500" ></p>

Selective Search Algorithm에 대해서 간단히 설명하면 물체의 스케일이나 위치를 모두 고려하여 모든 후보군을 처음에 샅샅이 조사한 다음, Greedy algorithm을 이용해서 여러 영역에서 비슷한 부분을 합치는 식으로 영역을 통합하는 방법이다.  

이제 R-CNN에 대해서 알아보자!

먼저 방금 언급한 Selective Search  Algorithm을 관심 영역을 추출한다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91523330-6d08fc80-e937-11ea-81c6-ce9f74228ba9.png" width = "500" ></p>

다음 CNN에 training을 시키기 위해 input size를 맞춰준다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91523594-1a7c1000-e938-11ea-93a9-447957290674.png" width = "500" ></p>

CNN를 통과한 다음 SVM을 통해서 classification과정을 거친다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91523647-44353700-e938-11ea-8918-932bfbdca446.png" width = "500" ></p>

다음 Linear Regression을 통해서 Bounding Box도 Loss 값을 계산하고,  
두개의 Loss가 최소가 되는 방향으로 training을 시킨다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91523754-82caf180-e938-11ea-8c6e-3630314a1eee.png" width = "500" ></p>

지금까지 R-CNN이 어떻게 동작하는지 살펴보았는데,  
R-CNN은 딥러닝을 이용한 object detection의 초창기 모델이라 다음과 같은 단점들이 존재한다.  

> - 계산 비용이 많이 든다.
> - 용량(Memory)도 많이 든다.
> - 학습 과정도 느리다. (논문 기준 81시간)
> - Test Time도 느리다. (한 이미지당 30초)
> - 학습이 되지 않는 Region Proposal이 존재

이러한 단점을 보완하기 위해 나온 알고리즘이 **Fast R-CNN**이다.

### Fast R-CNN

계산 비용이 많이 드는 이유는 각각의 ROI 영역에서 CNN 과정을 진행했기 때문이라고 생각했다.  

따라서 Fast R-CNN에서는 Input Image에 대해 한번의 CNN 과정을 거친다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91524030-1dc3cb80-e939-11ea-8942-878290f75041.png" width = "400" ></p>

그 다음 Feature Map에서 Region Proposal을 찾아낸다. 

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91524351-f15c7f00-e939-11ea-9a49-57429ee92e9a.png" width = "500" ></p>

이때 ROI의 영역에 모두 다른 Issue가 발생한다.  
이것이 왜 문제가 되냐면 일반적인 방법으로 Pooling을 통해 Input Size를 맞춰주어야 하는데 ROI가 다르기 때문에 일반적인 Pooling으로는 Input Size가 맞춰지지 않기 때문이다.  

따라서 ROI pooling이라는 새로운 기법을 이용하여 Feature map의 Size를 통일한다. 
<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91524557-78115c00-e93a-11ea-9567-e4734d8fe623.png" width = "500" ></p> 

그 다음 Fully Connected Layer를 통해서 Classification에 대한 Loss 값을 계산한다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91524632-a68f3700-e93a-11ea-891b-0b103bf33f5b.png" width = "500" ></p>

마찬가지로 Bounding Box에 대한 Loss 값도 계산한다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91524678-c32b6f00-e93a-11ea-9511-b635cd0fe381.png" width = "500" ></p>

R-CNN과 Fast R-CNN을 시간적인 측면에서 비교를 해보면

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91524828-2c12e700-e93b-11ea-9e9c-5dc7ee193711.png" width = "600" ></p>

참고로 SPP-Net은 R-CNN과 Fast R-CNN의 중간 정도의 성능을 가진 알고리즘이라 생각하면 된다.  
한 눈에 보아도 Fast R-CNN이 시간이 훨씬 단축된 것을 볼 수 있다.  

빨간색 막대가 나타내는 것은 Selective Search Algorithm이 수행되는데 걸리는 시간을  
제외한 시간이다.

Rule-based algorithm이라 그런지 시간이 많이 걸리는 것을 알 수 있다.  

그렇다면 Region Proposal도 딥러닝으로 수행하면 시간을 단축시키지 않을까?

그래서 나온 알고리즘이 **Faster R-CNN**이다.
(이름을 참 대충 지었다.ㅋ)

### Faster R-CNN

Faster R-CNN은 Feature map을 만드는 CNN과 그 이후 classification + localization하는데 사용하는 CNN을 공유해서 사용하자는 아이디어에서 출발했다.  

따라서 총 4개의 Loss function이 필요하다.
> 1. RPN classify object / not object (binary classification)
> 2. RPN regress box coordinates
> 3. Final classification score (object classes)
> 4. Final box coordinates

RPN은 Region Proposal Network을 의미한다.  
Faster R-CNN에서는 총 9개의 anchor box를 사용하는데

Anchor box란 물체의 모양이 어떻게 생겼는지 모르니 서로 다른 비율의 Bounding Box를 만들고, 모두 시도를 해봐서 최적의 Bounding Box를 찾기 위해 사용하는 것이라 생각하면 이해하기 쉽다.  

물체가 있을 확률을 $IoU$(Intersection over Union)을 통해 계산한다.

내가 예측한 Boundnig Box의 크기와 실제 Groundtruth의 값을 비교해보는 것이다.
$$ IoU = \frac {anchor \cap GroundTruth Box} {anchor \cup GroundTruth Box}  $$ 

그림으로 Faster R-CNN을 나타내면 다음과 같다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91525852-5c5b8500-e93d-11ea-8885-0cf5273b57c6.png" width = "400" ></p>

이 부분을 제외하고 나머지 부분은 Fast R-CNN과 거의 유사하다.  

그렇다면 Faster R-CNN이 얼마나 빨라졌는지 알아보면

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91525960-8f057d80-e93d-11ea-9443-3c747265c362.png" width = "500" ></p>

확실히 빨라진 것을 볼 수 있다.

### Mask R-CNN

Mask R-CNN은 Image segmentation을 수행하기 위해 고안된 모델이다.

하지만 구조가 Faster R- CNN에서 upgrade 된 구조이기 때문에 Faster R-CNN을 설명한 다음 설명을 하려 한다.

Mask R-CNN은 기존의 Faster R-CNN을 Object detection 역할을 하도록 하고 각각의 RoI에 mask segmentation을 해주는 작은 FCN (Fully Convolutional Network)를 추가해주었다.  

Faster R-CNN과 다른 점은 크게 3가지가 있다.
> - Faster R-CNN에 존재하는 "bounding box regression branch"에 
병렬로 "object mask prediction branch" 추가.
> - ROI Pooling 대신 ROI Align 을 사용함.
> - Mask prediction 과 class prediction 을 decouple 함. (클래스 상관없이 masking)

- Object mask prediction branch
각 픽셀이 오브젝트에 해당하는 것인지 아닌지를 마스킹하는 네트워크(CNN)를 추가했다고 생각하면 된다.

- ROI Align
기존 Faster R-CNN에서 사용한 ROI pooling의 같은 경우 정확한 위치 정보를 담는 것이 크게 중요하지 않았기 때문에 픽셀 좌표 값이 소수점으로 나올 경우 먼저 각 좌표를 반올림 해준 후 Pooling을 거치는데,  Pixel단위로 detection을 진행해야 하는 segmentation task에 대해서는 문제가 발생한다.

구조는 다음과 같다.

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91532781-c7ab5400-e949-11ea-9fb7-93d2f3d38f5a.png" width = "500" ></p>



### Yolo / SSD

이제 2-stage Detector에 대한 이야기가 끝났으니  
1-stage Detector에 대한 이야기를 해보자.  

1-stage Detector는 크게 2종류로 나눈다.  
> - Yolo
> - SSD

하지만 이 강의에서는 두 종류의 알고리즘을 그냥 한번에 설명하고 넘어간다.  

<p align="center"><img src="https://user-images.githubusercontent.com/41863759/91526122-e1df3500-e93d-11ea-99d0-ba5429ee808e.png" width = "600" ></p>

앞서 설명했지만 간단히 설명하면 Classification과 Localization을 한 번의 네트워크에서 해결하는 것을 말한다.  
Bounding Box를 찾으면서 Confidence라는 변수를 통해 어떤 클래스에 들어가는 지도 예측을 진행한다.  

Yolo와 SSD의 차이점을 간단하게 설명하면  
Yolo같은 경우는 이미지의 Grid를 나눌 때 7*7로 고정을 한 다음,  
Bounding Box를 찾지만

SSD의 같은 경우 Grid를 나눌 때, 여러 feature map을 참고하여 세분화하여 Grid를 나누고 Bounding Box를 찾는다.  

자세한 내용은 다음의 링크를 참고해서 읽어보면 도움이 될 것이다.  

- Yolo 리뷰 : [갈아먹는 Object Detection [5] Yolo: You Only Look Once](https://yeomko.tistory.com/19)

- SSD 논문 분석 : [SSD: Single Shot Multibox Detector 분석](https://taeu.github.io/paper/deeplearning-paper-ssd/)

 
















